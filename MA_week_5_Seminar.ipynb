{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RByeKCfdaSZ7"
      },
      "source": [
        "# LELA70331 Computational Linguistics Week 5\n",
        "\n",
        "This week we are going to take a close look in some details 1-layer neural networks, also known as perceptrons. These were introduced to you in abstract in the lecture and in this seminar we are going to look at how they work in reality. Towards the end of the lecture we are going to build on this to look at multi-class single layer and multi-layer neural networks too.\n",
        "\n",
        "Perceptrons are commonly used as binary classifiers - applying one of two possible labels to input. The example that we are going to look at today is sentiment classification, where we classify a text as having either a \"negative\" or \"positive\" perspective on whatever it is discussing, e.g. a product it is reviewing.\n",
        "\n",
        "Note: the code is heavily based on examples in Chapter 3 of Rao, D., & McMahan, B. (2019). Natural language processing with PyTorch: build intelligent language applications using deep learning. O'Reilly Media, Inc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "x1U85sASKSul",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4720f9eb-b924-4658-dbfc-87344c6f0915"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: https://www.dropbox.com/s/x48j2nasbmm7l41/MA_CL_Week_5_Materials.zip?dl=0: No such file or directory\n",
            "unzip:  cannot find or open MA_CL_Week_5_Materials.zip, MA_CL_Week_5_Materials.zip.zip or MA_CL_Week_5_Materials.zip.ZIP.\n",
            "cp: cannot stat 'CL_Week_5_Materials/*': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!https://www.dropbox.com/s/x48j2nasbmm7l41/MA_CL_Week_5_Materials.zip?dl=0\n",
        "!unzip -q MA_CL_Week_5_Materials.zip\n",
        "!cp -r CL_Week_5_Materials/* .\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpnWOxajoiYY"
      },
      "source": [
        "### Importing modules"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGcsUizUB7b6"
      },
      "source": [
        "The most important thing we are importing here is PyTorch (https://pytorch.org/). This is one of the two widely used neural network/deep learning packages, the other being TensorFlow (https://www.tensorflow.org/). Both are great! We use PyTorch here because we have to choose and it is slightly more intuitive when first encountered."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "UqFmkcr0oiYY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "outputId": "8a6a0c05-fd97-4da1-e647-f504aa92ca76"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-5cb27f540b49>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnn_tools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVocabulary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mReviewVectorizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mReviewDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mReviewClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnn_tools2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnn_tools_gen\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nn_tools'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "from argparse import Namespace\n",
        "from collections import Counter\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm_notebook\n",
        "from nn_tools import Vocabulary, ReviewVectorizer, ReviewDataset, ReviewClassifier\n",
        "from nn_tools2 import *\n",
        "from nn_tools_gen import *\n",
        "from mlp_tools import *\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXOjuJWa9B93"
      },
      "source": [
        "###Organizing code in Python (a very brief intro)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFdxhRcsGqMl"
      },
      "source": [
        "### Functions \n",
        "The first thing you will notice is that we are starting to define our own functions (https://www.w3schools.com/python/python_functions.asp):\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YPN4KZXEGpjK"
      },
      "outputs": [],
      "source": [
        "def tell_my_name(name):\n",
        "  return(\"My name is \" + name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WdU5j_UlHH-X"
      },
      "outputs": [],
      "source": [
        "print(tell_my_name(\"Colin\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_SXW3n39tZh"
      },
      "source": [
        "### Objects\n",
        "\n",
        "A second thing you will see is that we start to define our own objects (https://www.w3schools.com/python/python_classes.asp):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xGT7CFpK_uCX"
      },
      "outputs": [],
      "source": [
        "class Agent:\n",
        "    def tell_my_name(name):\n",
        "        return(\"My name is \" + name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pn002UWEArna"
      },
      "outputs": [],
      "source": [
        "print(Agent.tell_my_name(\"Colin\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QoNTVeOS_up0"
      },
      "source": [
        "As well as being a way to organize functions this can be a way to store and group variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g4QChM-S94p-"
      },
      "outputs": [],
      "source": [
        "class Agent:\n",
        "    def __init__(self, name=\"\",workplace=\"\"):\n",
        "        self.name=name\n",
        "        self.workplace=workplace\n",
        "    def introduce_myself(self):\n",
        "        return(\"My name is \" + self.name + \" and I work in \" + self.workplace)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ro3-BIHF-UyU"
      },
      "outputs": [],
      "source": [
        "agent_colin = Agent(\"Colin\",\"Manchester\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tR_H9ohf_JRt"
      },
      "outputs": [],
      "source": [
        "print(agent_colin.introduce_myself())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMi_4fF8Zp3N"
      },
      "source": [
        "## Single Layer Networks in PyTorch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_PI2ySZBSsJ"
      },
      "source": [
        "### Defining our Single layer network (aka Perceptron)\n",
        "\n",
        "We need to define our object type Perceptron. In doing so we make use of perhaps the most powerful property of objects with is inheritance - we define Perceptron as a subclass of the PyTorch object nn.Module and thereby inherit all of the attributes and functions from that object type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQA_P6NyEyYR"
      },
      "outputs": [],
      "source": [
        "class Perceptron(nn.Module):\n",
        "    \"\"\" A Perceptron is one Linear layer \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input_dim (int): size of the input features\n",
        "        \"\"\"\n",
        "        super(Perceptron, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 1)\n",
        "\n",
        "    def forward(self, x_in):\n",
        "        \"\"\"The forward pass of the MLP\n",
        "\n",
        "        Args:\n",
        "            x_in (torch.Tensor): an input data tensor. \n",
        "                x_in.shape should be (batch, input_dim)\n",
        "        Returns:\n",
        "            the resulting tensor. tensor.shape should be (batch, 1)\n",
        "        \"\"\"\n",
        "        return torch.sigmoid(self.fc1(x_in))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yud0AcZ8oiYW"
      },
      "source": [
        "## Classifying Yelp Reviews\n",
        "\n",
        "We have 56000 reviews on Yelp classified as negative (1 or 2 star) or positive (3 or 4 star). We are going to train a classifier using this a part of this data and test its performance on another part."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1ek7yp-oiYg"
      },
      "source": [
        "### Settings and some prep work\n",
        "\n",
        "We first load in our data, and set some parameters for use in training. We are also going to import a pretrained model (to save time, although there is code for model training below if you want to try this out)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pCssGfrRoiYg"
      },
      "outputs": [],
      "source": [
        "args = Namespace(\n",
        "    # Data and Path information\n",
        "    frequency_cutoff=25,\n",
        "    model_state_file='model.pth',\n",
        "    review_csv='reviews_with_splits_lite.csv',\n",
        "    save_dir='.',\n",
        "    vectorizer_file='vectorizer.json',\n",
        "    # No Model hyper parameters\n",
        "    # Training hyper parameters\n",
        "    batch_size=128,\n",
        "    early_stopping_criteria=5,\n",
        "    learning_rate=0.001,\n",
        "    num_epochs=100,\n",
        "    seed=1337,\n",
        "    # Runtime options\n",
        "    catch_keyboard_interrupt=True,\n",
        "    cuda=True,\n",
        "    expand_filepaths_to_save_dir=True,\n",
        "    reload_from_files=False,\n",
        ")\n",
        "\n",
        "if args.expand_filepaths_to_save_dir:\n",
        "    args.vectorizer_file = os.path.join(args.save_dir,\n",
        "                                        args.vectorizer_file)\n",
        "\n",
        "    args.model_state_file = os.path.join(args.save_dir,\n",
        "                                         args.model_state_file)\n",
        "    \n",
        "    print(\"Expanded filepaths: \")\n",
        "    print(\"\\t{}\".format(args.vectorizer_file))\n",
        "    print(\"\\t{}\".format(args.model_state_file))\n",
        "    \n",
        "# Check CUDA\n",
        "if not torch.cuda.is_available():\n",
        "    args.cuda = False\n",
        "\n",
        "print(\"Using CUDA: {}\".format(args.cuda))\n",
        "\n",
        "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
        "\n",
        "# Set seed for reproducibility\n",
        "set_seed_everywhere(args.seed, args.cuda)\n",
        "\n",
        "# handle dirs\n",
        "handle_dirs(args.save_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GXo-xNyzc9ke"
      },
      "outputs": [],
      "source": [
        "reviews = pd.read_csv(args.review_csv)\n",
        "print(reviews)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXCyTwt_oiYi"
      },
      "source": [
        "### Initializations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n4Y_5EknoiYi"
      },
      "outputs": [],
      "source": [
        "if args.reload_from_files:\n",
        "    # training from a checkpoint\n",
        "    print(\"Loading dataset and vectorizer\")\n",
        "    dataset = ReviewDataset.load_dataset_and_load_vectorizer(args.review_csv,\n",
        "                                                            args.vectorizer_file)\n",
        "else:\n",
        "    print(\"Loading dataset and creating vectorizer\")\n",
        "    # create dataset and vectorizer\n",
        "    dataset = ReviewDataset.load_dataset_and_make_vectorizer(args.review_csv)\n",
        "    dataset.save_vectorizer(args.vectorizer_file)    \n",
        "vectorizer = dataset.get_vectorizer()\n",
        "\n",
        "classifier = ReviewClassifier(num_features=len(vectorizer.review_vocab))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etKuqCZqEJEn"
      },
      "source": [
        "we are going to use what is known as one-hot coding (in statistics it is called dummy coding) for words. For a vocab of size N we have N dimensions. Each dimension has the value of 1 for a single word and zero for all others. For example, the first 50 dimensions for the word \"can\" look like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CA8w7He2sPoo"
      },
      "outputs": [],
      "source": [
        "vectorizer.vectorize('can')[0:50]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVfJ3jY0GPMc"
      },
      "source": [
        "Data is then input to the Perceptron in this format and weights learned for each dimension"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HIIE7R_yGOD2"
      },
      "outputs": [],
      "source": [
        "classifier.fc1.weight.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJw5KYCSGdn_"
      },
      "outputs": [],
      "source": [
        "classifier.fc1.weight"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtgFE2I1oiYj"
      },
      "source": [
        "### Classifying instances\n",
        "\n",
        "We now define a function predict_rating which will allow us to assign labels to previously unseen reviews."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aEZnPllwoiYk"
      },
      "outputs": [],
      "source": [
        "def predict_rating(review, classifier, vectorizer, decision_threshold=0.5):\n",
        "    \"\"\"Predict the rating of a review\n",
        "    \n",
        "    Args:\n",
        "        review (str): the text of the review\n",
        "        classifier (ReviewClassifier): the trained model\n",
        "        vectorizer (ReviewVectorizer): the corresponding vectorizer\n",
        "        decision_threshold (float): The numerical boundary which separates the rating classes\n",
        "    \"\"\"\n",
        "    review = preprocess_text(review)\n",
        "    \n",
        "    vectorized_review = torch.tensor(vectorizer.vectorize(review))\n",
        "    result = classifier(vectorized_review.view(1, -1))\n",
        "    \n",
        "    probability_value = torch.sigmoid(result).item()\n",
        "    index = 1\n",
        "    if probability_value < decision_threshold:\n",
        "        index = 0\n",
        "\n",
        "    return vectorizer.rating_vocab.lookup_index(index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "peRbWqSaoiYk"
      },
      "outputs": [],
      "source": [
        "test_review = \"this is a pretty awesome book\"\n",
        "\n",
        "classifier = classifier.cpu()\n",
        "prediction = predict_rating(test_review, classifier, vectorizer, decision_threshold=0.5)\n",
        "print(\"{} -> {}\".format(test_review, prediction))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b7e1erdWUCHh"
      },
      "outputs": [],
      "source": [
        "test_review = \"this is a pretty terrible book\"\n",
        "\n",
        "classifier = classifier.cpu()\n",
        "prediction = predict_rating(test_review, classifier, vectorizer, decision_threshold=0.5)\n",
        "print(\"{} -> {}\".format(test_review, prediction))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbOfq6w3oiYi"
      },
      "source": [
        "### Run on Test Data\n",
        "\n",
        "To evaluate overall performance we can run on our test data and compare the ratings with our annotations in order to calculate an accuracy score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9t9rPf-QoiYj"
      },
      "outputs": [],
      "source": [
        "# compute the loss & accuracy on the test set using the best available model\n",
        "loss_func = nn.BCEWithLogitsLoss()\n",
        "train_state = make_train_state(args)\n",
        "classifier.load_state_dict(torch.load(train_state['model_filename']))\n",
        "classifier = classifier.to(args.device)\n",
        "\n",
        "dataset.set_split('test')\n",
        "batch_generator = generate_batches(dataset, \n",
        "                                   batch_size=args.batch_size, \n",
        "                                   device=args.device)\n",
        "running_loss = 0.\n",
        "running_acc = 0.\n",
        "classifier.eval()\n",
        "\n",
        "for batch_index, batch_dict in enumerate(batch_generator):\n",
        "    # compute the output\n",
        "    y_pred = classifier(x_in=batch_dict['x_data'].float())\n",
        "\n",
        "    # compute the loss\n",
        "    loss = loss_func(y_pred, batch_dict['y_target'].float())\n",
        "    loss_t = loss.item()\n",
        "    running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
        "\n",
        "    # compute the accuracy\n",
        "    acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
        "    running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
        "\n",
        "train_state['test_loss'] = running_loss\n",
        "train_state['test_acc'] = running_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cNpCZfJgoiYj"
      },
      "outputs": [],
      "source": [
        "print(\"Test loss: {:.3f}\".format(train_state['test_loss']))\n",
        "print(\"Test Accuracy: {:.2f}\".format(train_state['test_acc']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nF92vWHoiYk"
      },
      "source": [
        "### Interpretability\n",
        "\n",
        "The simplicity of the Perceptron (it only has 1 layer) means that it is straightforward to interpret by looking at model weights. When models have more layers (or even once we start using representations other than one hot encoding) this becomes much more difficult!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "huHrwlgHoiYl"
      },
      "outputs": [],
      "source": [
        "# Sort weights\n",
        "fc1_weights = classifier.fc1.weight.detach()[0]\n",
        "_, indices = torch.sort(fc1_weights, dim=0, descending=True)\n",
        "indices = indices.cpu().numpy().tolist()\n",
        "\n",
        "# Top 20 words\n",
        "print(\"Influential words in Positive Reviews:\")\n",
        "print(\"--------------------------------------\")\n",
        "for i in range(20):\n",
        "    print(vectorizer.review_vocab.lookup_index(indices[i]))\n",
        "    \n",
        "print(\"====\\n\\n\\n\")\n",
        "\n",
        "# Top 20 negative words\n",
        "print(\"Influential words in Negative Reviews:\")\n",
        "print(\"--------------------------------------\")\n",
        "indices.reverse()\n",
        "for i in range(20):\n",
        "    print(vectorizer.review_vocab.lookup_index(indices[i]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyKVjflaoiYl"
      },
      "source": [
        "### Training Loop\n",
        "\n",
        "Here, for completeness, is the loop that was used for model training. You can run it, but it will take a while. \n",
        "\n",
        "If you do this, it will run faster if you switch to using a processor type know as a GPU. You can do this as follows:\n",
        "\n",
        "Navigate to Edit→Notebook Settings\n",
        "select GPU from the Hardware Accelerator drop-down"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0PvhWBYooiYi"
      },
      "outputs": [],
      "source": [
        "classifier = classifier.to(args.device)\n",
        "\n",
        "loss_func = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
        "                                                 mode='min', factor=0.5,\n",
        "                                                 patience=1)\n",
        "\n",
        "train_state = make_train_state(args)\n",
        "\n",
        "epoch_bar = tqdm_notebook(desc='training routine', \n",
        "                          total=args.num_epochs,\n",
        "                          position=0)\n",
        "\n",
        "dataset.set_split('train')\n",
        "train_bar = tqdm_notebook(desc='split=train',\n",
        "                          total=dataset.get_num_batches(args.batch_size), \n",
        "                          position=1, \n",
        "                          leave=True)\n",
        "dataset.set_split('val')\n",
        "val_bar = tqdm_notebook(desc='split=val',\n",
        "                        total=dataset.get_num_batches(args.batch_size), \n",
        "                        position=1, \n",
        "                        leave=True)\n",
        "\n",
        "try:\n",
        "    for epoch_index in range(args.num_epochs):\n",
        "        train_state['epoch_index'] = epoch_index\n",
        "\n",
        "        # Iterate over training dataset\n",
        "\n",
        "        # setup: batch generator, set loss and acc to 0, set train mode on\n",
        "        dataset.set_split('train')\n",
        "        batch_generator = generate_batches(dataset, \n",
        "                                           batch_size=args.batch_size, \n",
        "                                           device=args.device)\n",
        "        running_loss = 0.0\n",
        "        running_acc = 0.0\n",
        "        classifier.train()\n",
        "\n",
        "        for batch_index, batch_dict in enumerate(batch_generator):\n",
        "            # the training routine is these 5 steps:\n",
        "\n",
        "            # --------------------------------------\n",
        "            # step 1. zero the gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # step 2. compute the output\n",
        "            y_pred = classifier(x_in=batch_dict['x_data'].float())\n",
        "\n",
        "            # step 3. compute the loss\n",
        "            loss = loss_func(y_pred, batch_dict['y_target'].float())\n",
        "            loss_t = loss.item()\n",
        "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
        "\n",
        "            # step 4. use loss to produce gradients\n",
        "            loss.backward()\n",
        "\n",
        "            # step 5. use optimizer to take gradient step\n",
        "            optimizer.step()\n",
        "            # -----------------------------------------\n",
        "            # compute the accuracy\n",
        "            acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
        "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
        "\n",
        "            # update bar\n",
        "            train_bar.set_postfix(loss=running_loss, \n",
        "                                  acc=running_acc, \n",
        "                                  epoch=epoch_index)\n",
        "            train_bar.update()\n",
        "\n",
        "        train_state['train_loss'].append(running_loss)\n",
        "        train_state['train_acc'].append(running_acc)\n",
        "\n",
        "        # Iterate over val dataset\n",
        "\n",
        "        # setup: batch generator, set loss and acc to 0; set eval mode on\n",
        "        dataset.set_split('val')\n",
        "        batch_generator = generate_batches(dataset, \n",
        "                                           batch_size=args.batch_size, \n",
        "                                           device=args.device)\n",
        "        running_loss = 0.\n",
        "        running_acc = 0.\n",
        "        classifier.eval()\n",
        "\n",
        "        for batch_index, batch_dict in enumerate(batch_generator):\n",
        "\n",
        "            # compute the output\n",
        "            y_pred = classifier(x_in=batch_dict['x_data'].float())\n",
        "\n",
        "            # step 3. compute the loss\n",
        "            loss = loss_func(y_pred, batch_dict['y_target'].float())\n",
        "            loss_t = loss.item()\n",
        "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
        "\n",
        "            # compute the accuracy\n",
        "            acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
        "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
        "            \n",
        "            val_bar.set_postfix(loss=running_loss, \n",
        "                                acc=running_acc, \n",
        "                                epoch=epoch_index)\n",
        "            val_bar.update()\n",
        "\n",
        "        train_state['val_loss'].append(running_loss)\n",
        "        train_state['val_acc'].append(running_acc)\n",
        "\n",
        "        train_state = update_train_state(args=args, model=classifier,\n",
        "                                         train_state=train_state)\n",
        "\n",
        "        scheduler.step(train_state['val_loss'][-1])\n",
        "\n",
        "        train_bar.n = 0\n",
        "        val_bar.n = 0\n",
        "        epoch_bar.update()\n",
        "\n",
        "        if train_state['stop_early']:\n",
        "            break\n",
        "\n",
        "        train_bar.n = 0\n",
        "        val_bar.n = 0\n",
        "        epoch_bar.update()\n",
        "except KeyboardInterrupt:\n",
        "    print(\"Exiting loop\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi-class classification\n",
        "\n",
        "So far we have been looking at binary classification where the output has one of two possible values. However it is also possible to build neural networks that classify input into one of a set of any number of classes. We are going to look at a such a task.\n",
        "\n",
        "The only real difference between the previous task (assigning positive or negative sentiment) and this is that instead of two output classes, we now have 18. The only change we need to make is to the function applied to the output layer. Whereas before we had a single node in the output layer where we added up the weights and applied the sigmoid function to generate a probability of a positive label (and then we just subtract this value from 1 to give the probability of a negative label), now we have as many nodes as we have outcome labels and we generate a probability for each by applying the \"softmax\" function. The only thing you really need to know about this is that it assigns a probability to each of the classes, and the set of probabilities all sum to 1. If you are curious (and you don't need to be) you can find more information on the function used here: https://en.wikipedia.org/wiki/Softmax_function\n",
        "\n",
        "The dataset we are going to work with today consists of 10,000 surnames, labelled with 18 different nationalities. Our first tasks will be to learn a classifier that can accurately assign a label to previously unseen surnames (note: the task is inherently very noisy). The dataset is split into 70% training data (from which we learn our network weights), 15% validation (which we use to evaluate and tune our network weights) and 15% test (which we use to quantitatively evaluate performance)."
      ],
      "metadata": {
        "id": "7Tsws1wNU739"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multiclass Single layer neural network\n",
        "First we will use a single layer neural network. This is identical to the network that we used last week for sentiment classification, except for the output layer (that now applies a softmax function rather than a sigmoid function, as we have moved from 2 to >2 output classes)."
      ],
      "metadata": {
        "id": "evhJWvKKXUSS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SurnameClassifier(nn.Module):\n",
        "    \"\"\" A 1-layer Multilayer Perceptron for classifying surnames \"\"\"\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input_dim (int): the size of the input vectors\n",
        "            output_dim (int): the output size of the second Linear layer\n",
        "        \"\"\"\n",
        "        super(SurnameClassifier, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, output_dim)\n",
        "      \n",
        "    def forward(self, x_in, apply_softmax=False):\n",
        "        \"\"\"The forward pass of the classifier\n",
        "        \n",
        "        Args:\n",
        "            x_in (torch.Tensor): an input data tensor. \n",
        "                x_in.shape should be (batch, input_dim)\n",
        "            apply_softmax (bool): a flag for the softmax activation\n",
        "                should be false if used with the Cross Entropy losses\n",
        "        Returns:\n",
        "            the resulting tensor. tensor.shape should be (batch, output_dim)\n",
        "        \"\"\"\n",
        "        prediction_vector = self.fc1(x_in)\n",
        "\n",
        "        if apply_softmax:\n",
        "            prediction_vector = F.softmax(prediction_vector, dim=1)\n",
        "\n",
        "        return prediction_vector"
      ],
      "metadata": {
        "id": "uG8tm-syXjHd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args = Namespace(\n",
        "    # Data and path information\n",
        "    surname_csv=\"/content/gdrive/My Drive/CL_Week_5_Materials/surnames_with_splits.csv\",\n",
        "    vectorizer_file=\"slp_vectorizer.json\",\n",
        "    model_state_file=\"slp_model.pth\",\n",
        "    save_dir=\"/content/gdrive/My Drive/CL_Week_5_Materials/\",\n",
        "    # Model hyper parameters\n",
        "    hidden_dim=300,\n",
        "    # Training  hyper parameters\n",
        "    seed=1337,\n",
        "    num_epochs=100,\n",
        "    early_stopping_criteria=5,\n",
        "    learning_rate=0.001,\n",
        "    batch_size=64,\n",
        "    # Runtime options\n",
        "    cuda=False,\n",
        "    reload_from_files=False,\n",
        "    expand_filepaths_to_save_dir=True,\n",
        ")\n",
        "\n",
        "if args.expand_filepaths_to_save_dir:\n",
        "    args.vectorizer_file = os.path.join(args.save_dir,\n",
        "                                        args.vectorizer_file)\n",
        "\n",
        "    args.model_state_file = os.path.join(args.save_dir,\n",
        "                                         args.model_state_file)\n",
        "    \n",
        "    print(\"Expanded filepaths: \")\n",
        "    print(\"\\t{}\".format(args.vectorizer_file))\n",
        "    print(\"\\t{}\".format(args.model_state_file))\n",
        "    \n",
        "# Check CUDA\n",
        "if not torch.cuda.is_available():\n",
        "    args.cuda = False\n",
        "\n",
        "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
        "    \n",
        "print(\"Using CUDA: {}\".format(args.cuda))\n",
        "\n",
        "\n",
        "# Set seed for reproducibility\n",
        "set_seed_everywhere(args.seed, args.cuda)\n",
        "\n",
        "# handle dirs\n",
        "handle_dirs(args.save_dir)"
      ],
      "metadata": {
        "id": "JXAUpzmaXmtx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if args.reload_from_files:\n",
        "    # training from a checkpoint\n",
        "    print(\"Reloading!\")\n",
        "    dataset = SurnameDataset.load_dataset_and_load_vectorizer(args.surname_csv,\n",
        "                                                              args.vectorizer_file)\n",
        "else:\n",
        "    # create dataset and vectorizer\n",
        "    print(\"Creating fresh!\")\n",
        "    dataset = SurnameDataset.load_dataset_and_make_vectorizer(args.surname_csv)\n",
        "    dataset.save_vectorizer(args.vectorizer_file)\n",
        "    \n",
        "vectorizer = dataset.get_vectorizer()\n",
        "classifier = SurnameClassifier(input_dim=len(vectorizer.surname_vocab), \n",
        "                               hidden_dim=args.hidden_dim, \n",
        "                               output_dim=len(vectorizer.nationality_vocab))"
      ],
      "metadata": {
        "id": "hXY3F8_nXnWK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_state = make_train_state(args)\n",
        "classifier.load_state_dict(torch.load(train_state['model_filename']))\n",
        "\n",
        "classifier = classifier.to(args.device)\n",
        "dataset.class_weights = dataset.class_weights.to(args.device)\n",
        "loss_func = nn.CrossEntropyLoss(dataset.class_weights)\n",
        "\n",
        "dataset.set_split('test')\n",
        "batch_generator = generate_batches(dataset, \n",
        "                                   batch_size=args.batch_size, \n",
        "                                   device=args.device)\n",
        "running_loss = 0.\n",
        "running_acc = 0.\n",
        "classifier.eval()\n",
        "\n",
        "for batch_index, batch_dict in enumerate(batch_generator):\n",
        "    # compute the output\n",
        "    y_pred =  classifier(batch_dict['x_surname'])\n",
        "    \n",
        "    # compute the loss\n",
        "    loss = loss_func(y_pred, batch_dict['y_nationality'])\n",
        "    loss_t = loss.item()\n",
        "    running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
        "\n",
        "    # compute the accuracy\n",
        "    acc_t = compute_accuracy(y_pred, batch_dict['y_nationality'])\n",
        "    running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
        "    print(str(batch_index) + \" \" + str(running_loss) + \" \" + str(running_acc))\n",
        "\n",
        "train_state['test_loss'] = running_loss\n",
        "train_state['test_acc'] = running_acc"
      ],
      "metadata": {
        "id": "b7PtExsbXqNr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Test loss: {};\".format(train_state['test_loss']))\n",
        "print(\"Test Accuracy: {}\".format(train_state['test_acc']))"
      ],
      "metadata": {
        "id": "TQBRKA37XtZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_surname = input(\"Enter a surname to classify: \")\n",
        "classifier = classifier.to(\"cpu\")\n",
        "prediction = predict_nationality(new_surname, classifier, vectorizer)\n",
        "print(\"{} -> {} (p={:0.2f})\".format(new_surname,\n",
        "                                    prediction['nationality'],\n",
        "                                    prediction['probability']))"
      ],
      "metadata": {
        "id": "pq_3HzrgXxoT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcJu2x9Lq3O2"
      },
      "source": [
        "### MultiLayer Perceptron\n",
        "\n",
        "Next we will use a multilayer perceptron. This is identical to the model above except that we add a hidden layer between the input and the output layers that combined the weighted input from the different first layers (input) nodes and learns a new set of weights to apply to the resulting combined values before submitting these new values to the output (softmax) function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wR5JvoX2q2VU"
      },
      "outputs": [],
      "source": [
        "class SurnameClassifier(nn.Module):\n",
        "    \"\"\" A 2-layer Multilayer Perceptron for classifying surnames \"\"\"\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input_dim (int): the size of the input vectors\n",
        "            hidden_dim (int): the output size of the first Linear layer\n",
        "            output_dim (int): the output size of the second Linear layer\n",
        "        \"\"\"\n",
        "        super(SurnameClassifier, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x_in, apply_softmax=False):\n",
        "        \"\"\"The forward pass of the classifier\n",
        "        \n",
        "        Args:\n",
        "            x_in (torch.Tensor): an input data tensor. \n",
        "                x_in.shape should be (batch, input_dim)\n",
        "            apply_softmax (bool): a flag for the softmax activation\n",
        "                should be false if used with the Cross Entropy losses\n",
        "        Returns:\n",
        "            the resulting tensor. tensor.shape should be (batch, output_dim)\n",
        "        \"\"\"\n",
        "        intermediate_vector = F.relu(self.fc1(x_in))\n",
        "        prediction_vector = self.fc2(intermediate_vector)\n",
        "\n",
        "        if apply_softmax:\n",
        "            prediction_vector = F.softmax(prediction_vector, dim=1)\n",
        "\n",
        "        return prediction_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F2m7HbRmrLwy"
      },
      "outputs": [],
      "source": [
        "args = Namespace(\n",
        "    # Data and path information\n",
        "    surname_csv=\"/content/gdrive/My Drive/CL_Week_5_Materials/surnames_with_splits.csv\",\n",
        "    vectorizer_file=\"mlp_vectorizer.json\",\n",
        "    model_state_file=\"mlp_model.pth\",\n",
        "    save_dir=\"/content/gdrive/My Drive/CL_Week_5_Materials/\",\n",
        "    # Model hyper parameters\n",
        "    hidden_dim=300,\n",
        "    # Training  hyper parameters\n",
        "    seed=1337,\n",
        "    num_epochs=100,\n",
        "    early_stopping_criteria=5,\n",
        "    learning_rate=0.001,\n",
        "    batch_size=64,\n",
        "    # Runtime options\n",
        "    cuda=False,\n",
        "    reload_from_files=False,\n",
        "    expand_filepaths_to_save_dir=True,\n",
        ")\n",
        "\n",
        "if args.expand_filepaths_to_save_dir:\n",
        "    args.vectorizer_file = os.path.join(args.save_dir,\n",
        "                                        args.vectorizer_file)\n",
        "\n",
        "    args.model_state_file = os.path.join(args.save_dir,\n",
        "                                         args.model_state_file)\n",
        "    \n",
        "    print(\"Expanded filepaths: \")\n",
        "    print(\"\\t{}\".format(args.vectorizer_file))\n",
        "    print(\"\\t{}\".format(args.model_state_file))\n",
        "    \n",
        "# Check CUDA\n",
        "if not torch.cuda.is_available():\n",
        "    args.cuda = False\n",
        "\n",
        "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
        "    \n",
        "print(\"Using CUDA: {}\".format(args.cuda))\n",
        "\n",
        "\n",
        "# Set seed for reproducibility\n",
        "set_seed_everywhere(args.seed, args.cuda)\n",
        "\n",
        "# handle dirs\n",
        "handle_dirs(args.save_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yr6xbVLLrRU5"
      },
      "outputs": [],
      "source": [
        "if args.reload_from_files:\n",
        "    # training from a checkpoint\n",
        "    print(\"Reloading!\")\n",
        "    dataset = SurnameDataset.load_dataset_and_load_vectorizer(args.surname_csv,\n",
        "                                                              args.vectorizer_file)\n",
        "else:\n",
        "    # create dataset and vectorizer\n",
        "    print(\"Creating fresh!\")\n",
        "    dataset = SurnameDataset.load_dataset_and_make_vectorizer(args.surname_csv)\n",
        "    dataset.save_vectorizer(args.vectorizer_file)\n",
        "    \n",
        "vectorizer = dataset.get_vectorizer()\n",
        "classifier = SurnameClassifier(input_dim=len(vectorizer.surname_vocab), \n",
        "                               hidden_dim=args.hidden_dim, \n",
        "                               output_dim=len(vectorizer.nationality_vocab))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DygZuZENr1ke"
      },
      "outputs": [],
      "source": [
        "train_state = make_train_state(args)\n",
        "classifier.load_state_dict(torch.load(train_state['model_filename']))\n",
        "\n",
        "classifier = classifier.to(args.device)\n",
        "dataset.class_weights = dataset.class_weights.to(args.device)\n",
        "loss_func = nn.CrossEntropyLoss(dataset.class_weights)\n",
        "\n",
        "dataset.set_split('test')\n",
        "batch_generator = generate_batches(dataset, \n",
        "                                   batch_size=args.batch_size, \n",
        "                                   device=args.device)\n",
        "running_loss = 0.\n",
        "running_acc = 0.\n",
        "classifier.eval()\n",
        "\n",
        "for batch_index, batch_dict in enumerate(batch_generator):\n",
        "    # compute the output\n",
        "    y_pred =  classifier(batch_dict['x_surname'])\n",
        "    \n",
        "    # compute the loss\n",
        "    loss = loss_func(y_pred, batch_dict['y_nationality'])\n",
        "    loss_t = loss.item()\n",
        "    running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
        "\n",
        "    # compute the accuracy\n",
        "    acc_t = compute_accuracy(y_pred, batch_dict['y_nationality'])\n",
        "    running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
        "    print(str(batch_index) + \" \" + str(running_loss) + \" \" + str(running_acc))\n",
        "    \n",
        "train_state['test_loss'] = running_loss\n",
        "train_state['test_acc'] = running_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gh51rENgr4YH"
      },
      "outputs": [],
      "source": [
        "print(\"Test loss: {};\".format(train_state['test_loss']))\n",
        "print(\"Test Accuracy: {}\".format(train_state['test_acc']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iDw4dbe3r8Gy"
      },
      "outputs": [],
      "source": [
        "new_surname = input(\"Enter a surname to classify: \")\n",
        "classifier = classifier.to(\"cpu\")\n",
        "prediction = predict_nationality(new_surname, classifier, vectorizer)\n",
        "print(\"{} -> {} (p={:0.2f})\".format(new_surname,\n",
        "                                    prediction['nationality'],\n",
        "                                    prediction['probability']))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "toc": {
      "colors": {
        "hover_highlight": "#DAA520",
        "running_highlight": "#FF0000",
        "selected_highlight": "#FFD700"
      },
      "moveMenuLeft": true,
      "nav_menu": {
        "height": "156px",
        "width": "252px"
      },
      "navigate_menu": true,
      "number_sections": true,
      "sideBar": true,
      "threshold": "5",
      "toc_cell": false,
      "toc_section_display": "block",
      "toc_window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}