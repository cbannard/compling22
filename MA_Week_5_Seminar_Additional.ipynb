{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RByeKCfdaSZ7"
      },
      "source": [
        "# LELA70331 Computational Linguistics Week 5 -  Classifying Surnames\n",
        "\n",
        "Note: the code is heavily based on examples in Chapter 3 of Rao, D., & McMahan, B. (2019). Natural language processing with PyTorch: build intelligent language applications using deep learning. O'Reilly Media, Inc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v-HbCEVc3JoA"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x1U85sASKSul"
      },
      "outputs": [],
      "source": [
        "!wget https://www.dropbox.com/s/9dwmaiplltnnllp/CL_Week_5_Materials.zip\n",
        "!unzip -q CL_Week_5_Materials.zip\n",
        "!cp -r CL_Week_5_Materials /content/gdrive/My\\ Drive/\n",
        "!cp /content/gdrive/My\\ Drive/CL_Week_5_Materials/mlp_tools.py .\n",
        "!cp /content/gdrive/My\\ Drive/CL_Week_5_Materials/nn_tools_gen.py .\n",
        "!cp /content/gdrive/My\\ Drive/CL_Week_5_Materials/rnn_tools.py .\n",
        "!cp /content/gdrive/My\\ Drive/CL_Week_5_Materials/generation_tools.py ."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r __MACOSX/*"
      ],
      "metadata": {
        "id": "HqIRLp9XwtkO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpnWOxajoiYY"
      },
      "source": [
        "### Importing modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UqFmkcr0oiYY"
      },
      "outputs": [],
      "source": [
        "from argparse import Namespace\n",
        "from collections import Counter\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm_notebook\n",
        "from nn_tools_gen import *\n",
        "from mlp_tools import *\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMpDGWm25RkV"
      },
      "source": [
        "### Classifying surnames\n",
        "\n",
        "The dataset we are going to work with today consists of 10,000 surnames, labelled with 18 different nationalities. Our first tasks will be to learn a classifier that can accurately assign a label to previously unseen surnames (note: the task is inherently very noisy). Later we will look at generating surnames. The dataset is split into 70% training data (from which we learn our network weights), 15% validation (which we use to evaluate and tune our network weights) and 15% test (which we use to quantitatively evaluate performance).\n",
        "\n",
        "A critical difference between last week's task (assigning positive or negative sentiment) and this weeks is that instead of two output classes, we now have 18. The only change we need to make is to the function applied to the output layer. Whereas before we had a single node in the output layer where we added up the weights and applied the sigmoid function to generate a probability of a positive label (and then we just subtract this value from 1 to give the probability of a negative label), now we have as many nodes as we have outcome labels and we generate a probability for each by applying the \"softmax\" function. The only thing you really need to know about this is that it assigns a probability to each of the classes, and the set of probabilities all sum to 1. If you are curious (and you don't need to be) you can find more information on the function used here: https://en.wikipedia.org/wiki/Softmax_function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmQ7A-vR9MWH"
      },
      "source": [
        "### Multiclass Single layer neural network\n",
        "\n",
        "First we will use a single layer neural network. This is identical to the network that we used last week for sentiment classification, except for the output layer (that now applies a softmax function rather than a sigmoid function, as we have moved from 2 to >2 output classes)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tbck87LC-AKV"
      },
      "outputs": [],
      "source": [
        "class SurnameClassifier(nn.Module):\n",
        "    \"\"\" A 1-layer Multilayer Perceptron for classifying surnames \"\"\"\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input_dim (int): the size of the input vectors\n",
        "            output_dim (int): the output size of the second Linear layer\n",
        "        \"\"\"\n",
        "        super(SurnameClassifier, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, output_dim)\n",
        "      \n",
        "    def forward(self, x_in, apply_softmax=False):\n",
        "        \"\"\"The forward pass of the classifier\n",
        "        \n",
        "        Args:\n",
        "            x_in (torch.Tensor): an input data tensor. \n",
        "                x_in.shape should be (batch, input_dim)\n",
        "            apply_softmax (bool): a flag for the softmax activation\n",
        "                should be false if used with the Cross Entropy losses\n",
        "        Returns:\n",
        "            the resulting tensor. tensor.shape should be (batch, output_dim)\n",
        "        \"\"\"\n",
        "        prediction_vector = self.fc1(x_in)\n",
        "\n",
        "        if apply_softmax:\n",
        "            prediction_vector = F.softmax(prediction_vector, dim=1)\n",
        "\n",
        "        return prediction_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bXtobfIs-OJc"
      },
      "outputs": [],
      "source": [
        "args = Namespace(\n",
        "    # Data and path information\n",
        "    surname_csv=\"/content/gdrive/My Drive/CL_Week_5_Materials/surnames_with_splits.csv\",\n",
        "    vectorizer_file=\"slp_vectorizer.json\",\n",
        "    model_state_file=\"slp_model.pth\",\n",
        "    save_dir=\"/content/gdrive/My Drive/CL_Week_5_Materials/\",\n",
        "    # Model hyper parameters\n",
        "    hidden_dim=300,\n",
        "    # Training  hyper parameters\n",
        "    seed=1337,\n",
        "    num_epochs=100,\n",
        "    early_stopping_criteria=5,\n",
        "    learning_rate=0.001,\n",
        "    batch_size=64,\n",
        "    # Runtime options\n",
        "    cuda=False,\n",
        "    reload_from_files=False,\n",
        "    expand_filepaths_to_save_dir=True,\n",
        ")\n",
        "\n",
        "if args.expand_filepaths_to_save_dir:\n",
        "    args.vectorizer_file = os.path.join(args.save_dir,\n",
        "                                        args.vectorizer_file)\n",
        "\n",
        "    args.model_state_file = os.path.join(args.save_dir,\n",
        "                                         args.model_state_file)\n",
        "    \n",
        "    print(\"Expanded filepaths: \")\n",
        "    print(\"\\t{}\".format(args.vectorizer_file))\n",
        "    print(\"\\t{}\".format(args.model_state_file))\n",
        "    \n",
        "# Check CUDA\n",
        "if not torch.cuda.is_available():\n",
        "    args.cuda = False\n",
        "\n",
        "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
        "    \n",
        "print(\"Using CUDA: {}\".format(args.cuda))\n",
        "\n",
        "\n",
        "# Set seed for reproducibility\n",
        "set_seed_everywhere(args.seed, args.cuda)\n",
        "\n",
        "# handle dirs\n",
        "handle_dirs(args.save_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_Az7-X7-Swz"
      },
      "outputs": [],
      "source": [
        "if args.reload_from_files:\n",
        "    # training from a checkpoint\n",
        "    print(\"Reloading!\")\n",
        "    dataset = SurnameDataset.load_dataset_and_load_vectorizer(args.surname_csv,\n",
        "                                                              args.vectorizer_file)\n",
        "else:\n",
        "    # create dataset and vectorizer\n",
        "    print(\"Creating fresh!\")\n",
        "    dataset = SurnameDataset.load_dataset_and_make_vectorizer(args.surname_csv)\n",
        "    dataset.save_vectorizer(args.vectorizer_file)\n",
        "    \n",
        "vectorizer = dataset.get_vectorizer()\n",
        "classifier = SurnameClassifier(input_dim=len(vectorizer.surname_vocab), \n",
        "                               hidden_dim=args.hidden_dim, \n",
        "                               output_dim=len(vectorizer.nationality_vocab))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kgcqnl2L-hQu"
      },
      "outputs": [],
      "source": [
        "train_state = make_train_state(args)\n",
        "classifier.load_state_dict(torch.load(train_state['model_filename']))\n",
        "\n",
        "classifier = classifier.to(args.device)\n",
        "dataset.class_weights = dataset.class_weights.to(args.device)\n",
        "loss_func = nn.CrossEntropyLoss(dataset.class_weights)\n",
        "\n",
        "dataset.set_split('test')\n",
        "batch_generator = generate_batches(dataset, \n",
        "                                   batch_size=args.batch_size, \n",
        "                                   device=args.device)\n",
        "running_loss = 0.\n",
        "running_acc = 0.\n",
        "classifier.eval()\n",
        "\n",
        "for batch_index, batch_dict in enumerate(batch_generator):\n",
        "    # compute the output\n",
        "    y_pred =  classifier(batch_dict['x_surname'])\n",
        "    \n",
        "    # compute the loss\n",
        "    loss = loss_func(y_pred, batch_dict['y_nationality'])\n",
        "    loss_t = loss.item()\n",
        "    running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
        "\n",
        "    # compute the accuracy\n",
        "    acc_t = compute_accuracy(y_pred, batch_dict['y_nationality'])\n",
        "    running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
        "    print(str(batch_index) + \" \" + str(running_loss) + \" \" + str(running_acc))\n",
        "\n",
        "train_state['test_loss'] = running_loss\n",
        "train_state['test_acc'] = running_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tPGJ_-de-ms_"
      },
      "outputs": [],
      "source": [
        "print(\"Test loss: {};\".format(train_state['test_loss']))\n",
        "print(\"Test Accuracy: {}\".format(train_state['test_acc']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kOlQCx54-qvO"
      },
      "outputs": [],
      "source": [
        "new_surname = input(\"Enter a surname to classify: \")\n",
        "classifier = classifier.to(\"cpu\")\n",
        "prediction = predict_nationality(new_surname, classifier, vectorizer)\n",
        "print(\"{} -> {} (p={:0.2f})\".format(new_surname,\n",
        "                                    prediction['nationality'],\n",
        "                                    prediction['probability']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFLoMQ_FF6wm"
      },
      "source": [
        "If you want to test a few at a time you can put them in a list like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jxYgBV8hF5y5"
      },
      "outputs": [],
      "source": [
        "# surname = input(\"Enter a surname: \")\n",
        "classifier = classifier.to(\"cpu\")\n",
        "for surname in ['McMahan', 'Nakamoto', 'Wan', 'Cho']:\n",
        "    print(predict_nationality(surname, classifier, vectorizer))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXkiVE3bE0Vr"
      },
      "source": [
        "### Gated Recurrent Unit (GRU)-based classifier\n",
        "\n",
        "In order to overcome the limitations of the bag-of-characters representation we can use a recurrent neural network with gating. As explained in the lecture this uses gates at each step (of the input, e.g. words or characters) in order to learn what information to pass along through the recurrent connections. "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from rnn_tools import *"
      ],
      "metadata": {
        "id": "31ajUgybvZzq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9iwnFO-PE-8x"
      },
      "outputs": [],
      "source": [
        "class GRU(nn.Module):\n",
        "    \"\"\" a GRU network built using the GRU Cell \"\"\"\n",
        "    def __init__(self, input_size, hidden_size, batch_first=False):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input_size (int): size of the input vectors\n",
        "            hidden_size (int): size of the hidden state vectors\n",
        "            bathc_first (bool): whether the 0th dimension is batch\n",
        "        \"\"\"\n",
        "        super(GRU, self).__init__()\n",
        "        \n",
        "        self.rnn_cell = nn.GRUCell(input_size, hidden_size)\n",
        "        \n",
        "        self.batch_first = batch_first\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "    def _initial_hidden(self, batch_size):\n",
        "        return torch.zeros((batch_size, self.hidden_size))\n",
        "\n",
        "    def forward(self, x_in, initial_hidden=None):\n",
        "        \"\"\"The forward pass of the GRU\n",
        "        \n",
        "        Args:\n",
        "            x_in (torch.Tensor): an input data tensor. \n",
        "                If self.batch_first: x_in.shape = (batch, seq_size, feat_size)\n",
        "                Else: x_in.shape = (seq_size, batch, feat_size)\n",
        "            initial_hidden (torch.Tensor): the initial hidden state for the RNN\n",
        "        Returns:\n",
        "            hiddens (torch.Tensor): The outputs of the RNN at each time step. \n",
        "                If self.batch_first: hiddens.shape = (batch, seq_size, hidden_size)\n",
        "                Else: hiddens.shape = (seq_size, batch, hidden_size)\n",
        "        \"\"\"\n",
        "        if self.batch_first:\n",
        "            batch_size, seq_size, feat_size = x_in.size()\n",
        "            x_in = x_in.permute(1, 0, 2)\n",
        "        else:\n",
        "            seq_size, batch_size, feat_size = x_in.size()\n",
        "    \n",
        "        hiddens = []\n",
        "\n",
        "        if initial_hidden is None:\n",
        "            initial_hidden = self._initial_hidden(batch_size)\n",
        "            initial_hidden = initial_hidden.to(x_in.device)\n",
        "\n",
        "        hidden_t = initial_hidden\n",
        "                    \n",
        "        for t in range(seq_size):\n",
        "            hidden_t = self.rnn_cell(x_in[t], hidden_t)\n",
        "            hiddens.append(hidden_t)\n",
        "            \n",
        "        hiddens = torch.stack(hiddens)\n",
        "\n",
        "        if self.batch_first:\n",
        "            hiddens = hiddens.permute(1, 0, 2)\n",
        "\n",
        "        return hiddens\n",
        "\n",
        "\n",
        "\n",
        "class SurnameClassifier(nn.Module):\n",
        "    \"\"\" A Classifier with an RNN to extract features and an MLP to classify \"\"\"\n",
        "    def __init__(self, embedding_size, num_embeddings, num_classes,\n",
        "                 rnn_hidden_size, batch_first=True, padding_idx=0):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            embedding_size (int): The size of the character embeddings\n",
        "            num_embeddings (int): The number of characters to embed\n",
        "            num_classes (int): The size of the prediction vector \n",
        "                Note: the number of nationalities\n",
        "            rnn_hidden_size (int): The size of the RNN's hidden state\n",
        "            batch_first (bool): Informs whether the input tensors will \n",
        "                have batch or the sequence on the 0th dimension\n",
        "            padding_idx (int): The index for the tensor padding; \n",
        "                see torch.nn.Embedding\n",
        "        \"\"\"\n",
        "        super(SurnameClassifier, self).__init__()\n",
        "\n",
        "        self.emb = nn.Embedding(num_embeddings=num_embeddings,\n",
        "                                embedding_dim=embedding_size,\n",
        "                                padding_idx=padding_idx)\n",
        "        self.rnn = GRU(input_size=embedding_size,\n",
        "                             hidden_size=rnn_hidden_size,\n",
        "                             batch_first=batch_first)\n",
        "        self.fc1 = nn.Linear(in_features=rnn_hidden_size,\n",
        "                         out_features=rnn_hidden_size)\n",
        "        self.fc2 = nn.Linear(in_features=rnn_hidden_size,\n",
        "                          out_features=num_classes)\n",
        "\n",
        "    def forward(self, x_in, x_lengths=None, apply_softmax=False):\n",
        "        \"\"\"The forward pass of the classifier\n",
        "        \n",
        "        Args:\n",
        "            x_in (torch.Tensor): an input data tensor. \n",
        "                x_in.shape should be (batch, input_dim)\n",
        "            x_lengths (torch.Tensor): the lengths of each sequence in the batch.\n",
        "                They are used to find the final vector of each sequence\n",
        "            apply_softmax (bool): a flag for the softmax activation\n",
        "                should be false if used with the Cross Entropy losses\n",
        "        Returns:\n",
        "            the resulting tensor. tensor.shape should be (batch, output_dim)\n",
        "        \"\"\"\n",
        "        x_embedded = self.emb(x_in)\n",
        "        y_out = self.rnn(x_embedded)\n",
        "\n",
        "        if x_lengths is not None:\n",
        "            y_out = column_gather(y_out, x_lengths)\n",
        "        else:\n",
        "            y_out = y_out[:, -1, :]\n",
        "\n",
        "        y_out = F.relu(self.fc1(F.dropout(y_out, 0.5)))\n",
        "        y_out = self.fc2(F.dropout(y_out, 0.5))\n",
        "\n",
        "        if apply_softmax:\n",
        "            y_out = F.softmax(y_out, dim=1)\n",
        "\n",
        "        return y_out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1HQrO220E-_v"
      },
      "outputs": [],
      "source": [
        "args = Namespace(\n",
        "    # Data and path information\n",
        "    surname_csv=\"/content/gdrive/My Drive/CL_Week_5_Materials/surnames_with_splits.csv\",\n",
        "    vectorizer_file=\"gru_vectorizer.json\",\n",
        "    model_state_file=\"gru_model.pth\",\n",
        "    save_dir=\"/content/gdrive/My Drive/CL_Week_5_Materials/\",\n",
        "    # Model hyper parameter\n",
        "    char_embedding_size=100,\n",
        "    rnn_hidden_size=64,\n",
        "    # Training hyper parameter\n",
        "    num_epochs=100,\n",
        "    learning_rate=1e-3,\n",
        "    batch_size=64,\n",
        "    seed=1337,\n",
        "    early_stopping_criteria=5,\n",
        "    # Runtime hyper parameter\n",
        "    cuda=True,\n",
        "    catch_keyboard_interrupt=True,\n",
        "    reload_from_files=False,\n",
        "    expand_filepaths_to_save_dir=True,\n",
        ")\n",
        "\n",
        "# Check CUDA\n",
        "if not torch.cuda.is_available():\n",
        "    args.cuda = False\n",
        "\n",
        "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
        "    \n",
        "print(\"Using CUDA: {}\".format(args.cuda))\n",
        "\n",
        "\n",
        "if args.expand_filepaths_to_save_dir:\n",
        "    args.vectorizer_file = os.path.join(args.save_dir,\n",
        "                                        args.vectorizer_file)\n",
        "\n",
        "    args.model_state_file = os.path.join(args.save_dir,\n",
        "                                         args.model_state_file)\n",
        "    \n",
        "# Set seed for reproducibility\n",
        "set_seed_everywhere(args.seed, args.cuda)\n",
        "\n",
        "# handle dirs\n",
        "handle_dirs(args.save_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y_NPh5FcHBkj"
      },
      "outputs": [],
      "source": [
        "if args.reload_from_files and os.path.exists(args.vectorizer_file):\n",
        "    # training from a checkpoint\n",
        "    dataset = SurnameDataset.load_dataset_and_load_vectorizer(args.surname_csv, \n",
        "                                                              args.vectorizer_file)\n",
        "else:\n",
        "    # create dataset and vectorizer\n",
        "    dataset = SurnameDataset.load_dataset_and_make_vectorizer(args.surname_csv)\n",
        "    dataset.save_vectorizer(args.vectorizer_file)\n",
        "\n",
        "vectorizer = dataset.get_vectorizer()\n",
        "\n",
        "classifier = SurnameClassifier(embedding_size=args.char_embedding_size, \n",
        "                               num_embeddings=len(vectorizer.char_vocab),\n",
        "                               num_classes=len(vectorizer.nationality_vocab),\n",
        "                               rnn_hidden_size=args.rnn_hidden_size,\n",
        "                               padding_idx=vectorizer.char_vocab.mask_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EW1sauBvHIgN"
      },
      "outputs": [],
      "source": [
        "train_state = make_train_state(args)\n",
        "classifier.load_state_dict(torch.load(train_state['model_filename']))\n",
        "\n",
        "classifier = classifier.to(args.device)\n",
        "dataset.class_weights = dataset.class_weights.to(args.device)\n",
        "loss_func = nn.CrossEntropyLoss(dataset.class_weights)\n",
        "\n",
        "dataset.set_split('test')\n",
        "batch_generator = generate_batches(dataset, \n",
        "                                   batch_size=args.batch_size, \n",
        "                                   device=args.device)\n",
        "running_loss = 0.\n",
        "running_acc = 0.\n",
        "classifier.eval()\n",
        "\n",
        "for batch_index, batch_dict in enumerate(batch_generator):\n",
        "    # compute the output\n",
        "    y_pred =  classifier(batch_dict['x_data'],\n",
        "                         x_lengths=batch_dict['x_length'])\n",
        "    \n",
        "    # compute the loss\n",
        "    loss = loss_func(y_pred, batch_dict['y_target'])\n",
        "    loss_t = loss.item()\n",
        "    running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
        "\n",
        "    # compute the accuracy\n",
        "    acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
        "    running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
        "    print(str(batch_index) + \" \" + str(running_loss) + \" \" + str(running_acc))\n",
        "\n",
        "train_state['test_loss'] = running_loss\n",
        "train_state['test_acc'] = running_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RupB04stHJgT"
      },
      "outputs": [],
      "source": [
        "print(\"Test loss: {};\".format(train_state['test_loss']))\n",
        "print(\"Test Accuracy: {}\".format(train_state['test_acc']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4KUhiUdGHUQ5"
      },
      "outputs": [],
      "source": [
        "\n",
        "new_surname = input(\"Enter a surname to classify: \")\n",
        "classifier = classifier.to(\"cpu\")\n",
        "prediction = predict_nationality_rnn(new_surname, classifier, vectorizer)\n",
        "print(\"{} -> {} (p={:0.2f})\".format(new_surname,\n",
        "                                    prediction['nationality'],\n",
        "                                    prediction['probability']))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "toc": {
      "colors": {
        "hover_highlight": "#DAA520",
        "running_highlight": "#FF0000",
        "selected_highlight": "#FFD700"
      },
      "moveMenuLeft": true,
      "nav_menu": {
        "height": "156px",
        "width": "252px"
      },
      "navigate_menu": true,
      "number_sections": true,
      "sideBar": true,
      "threshold": "5",
      "toc_cell": false,
      "toc_section_display": "block",
      "toc_window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}