{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4_7KjskLX1q-"
   },
   "source": [
    "# LELA70331 Computational Linguistics Week 11\n",
    "\n",
    "This week we are going to take a look at Syntactic parsing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n72wT8QjotJe"
   },
   "source": [
    "We are going once again to use tools from NLTK, which we need to import as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-8VxIaVtb1Mz"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.parse.generate import generate\n",
    "from nltk import CFG, Tree\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3aoNQdlpo-u8"
   },
   "source": [
    "We can define phrase structure grammars using rewrite rules (see week 10 lecture for a definition) as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kbI6KouNhf5c"
   },
   "outputs": [],
   "source": [
    "grammar = nltk.CFG.fromstring(\"\"\"\n",
    "    S -> NP VP\n",
    "    NP -> Det N | Pronoun\n",
    "    VP -> V NP \n",
    "    Det -> 'the' \n",
    "    Pronoun -> 'I'\n",
    "    N -> 'dishes'  \n",
    "    V -> 'washed'\n",
    " \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PjuYbB6UpR_Y"
   },
   "source": [
    "We can then \"parse\" tokenised input sentences as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uei3wXryhnnv"
   },
   "outputs": [],
   "source": [
    "# define sentence and tokenize it\n",
    "sent = 'I washed the dishes'\n",
    "sent = nltk.word_tokenize(sent)\n",
    "# use a parser to generate all possible syntax trees for the input sentence given our grammar\n",
    "parser = nltk.ChartParser(grammar)\n",
    "# print out all analyses\n",
    "for tree in parser.parse(sent):\n",
    "    nltk.Tree.fromstring(str(tree)).pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C2m4CYAHvcQV"
   },
   "source": [
    "And we can generate from the grammar as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZMwVlntfPzuX"
   },
   "outputs": [],
   "source": [
    "for sentence in generate(grammar):\n",
    "     print(' '.join(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gCRbrUlgvjtK"
   },
   "source": [
    "Activity: Update the grammar so that it will parse \"They washed the car\". You can use the \"|\" symbol to allow multiple words or symbols on the right hand side of the rule, e.g. V -> 'washed' | 'threw'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U7n04N11yd81"
   },
   "source": [
    "Activity: Update the grammar so that it will parse \"The boy and his dog enter the park\". Note - it is permitted for the same terminal symbol to appear on the left and the right hand side of the same rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oERSzKlR3kLu"
   },
   "source": [
    "Activity: Generate from the grammar again. Why does it crash?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N0a2ibvwoWLd"
   },
   "source": [
    "Activity: Now we have fixed that, generate from the grammar again. What problems do you notice in the output? What might the solution be?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vt1DHCZRhwZs"
   },
   "source": [
    "Activity: Update the grammar so that it will correctly parse the sentence \"I washed the dishes on the counter\". The intended interpretation is that the dishes were formerly on the counter and the washing took place in the sink. So the correct parse is as follows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1DXY2GXPoOQW"
   },
   "source": [
    "<img src=\"https://drive.google.com/uc?id=160-bmjhw_Jk6FNBaCCOtJxyXuCL5I7GG\">\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YPWn6A3Sh8_A"
   },
   "outputs": [],
   "source": [
    "grammar = nltk.CFG.fromstring(\"\"\"\n",
    "    S -> NP VP\n",
    "    NP -> Det N | Pronoun\n",
    "    VP -> V NP \n",
    "    Det -> 'the' \n",
    "    Pronoun -> 'I'\n",
    "    N -> 'dishes' \n",
    "    V -> 'washed'\n",
    " \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rfrDwKvWiIYW"
   },
   "outputs": [],
   "source": [
    "sent = 'I washed the dishes on the counter'\n",
    "sent = nltk.word_tokenize(sent)\n",
    "parser = nltk.ChartParser(grammar)\n",
    "for tree in parser.parse(sent):\n",
    "    nltk.Tree.fromstring(str(tree)).pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cUD6dGGmwPV5"
   },
   "source": [
    "Activity: now add rules to the same grammar to also give the correct analysis to the sentence \"I washed my hair in the shower\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bpfBAEJuwseD"
   },
   "outputs": [],
   "source": [
    "sentences = ['I washed the dishes on the counter', 'I washed my hair in the shower']\n",
    "parser = nltk.ChartParser(grammar)\n",
    "for sent in sentences:\n",
    "    for tree in parser.parse(nltk.word_tokenize(sent)):\n",
    "        nltk.Tree.fromstring(str(tree)).pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YOX_q1_82Ab9"
   },
   "source": [
    "# Probabilistic Grammar\n",
    "Because even very simple grammars can allow multiple, and sometimes a great many, analyses for simple sentences, particularly as the grammar gets big, it becomes necessary to find a way to prefer one parse over others. One way to accomplish this is with probabilistic grammars where a weight is given to each rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fnJplKtw2Alr"
   },
   "outputs": [],
   "source": [
    "grammar = nltk.PCFG.fromstring(\"\"\"\n",
    "    S -> NP VP [1.0]\n",
    "    NP -> Det N [0.25]\n",
    "    NP -> NP PP [0.25]\n",
    "    NP -> N PP [0.25]\n",
    "    NP -> Pronoun [0.25]\n",
    "    PP -> P NP [1.0]\n",
    "    VP -> V NP [0.5]\n",
    "    VP -> VP PP [0.5]\n",
    "    Det -> 'the' [0.5]\n",
    "    Det -> 'my' [0.5]\n",
    "    Pronoun -> 'I' [1.0]\n",
    "    N -> 'dishes'  [0.25]\n",
    "    N -> 'sink' [0.25]\n",
    "    N -> 'breakfast' [0.25]\n",
    "    N -> 'pyjamas'[0.25]\n",
    "    V -> 'washed' [0.5]\n",
    "    V ->  'ate' [0.5]\n",
    "    P -> 'in' [1.0]\n",
    " \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OWCIXk2PwyqS"
   },
   "outputs": [],
   "source": [
    "sentences = ['I ate my breakfast in my pyjamas', 'I washed the dishes in the sink']\n",
    "parser = nltk.ViterbiParser(grammar)\n",
    "import re\n",
    "for sent in sentences:\n",
    "    for tree in parser.parse_all(nltk.word_tokenize(sent)):\n",
    "        tree = re.sub(\"\\(p[^\\)]+\\)\",\"\",str(tree))\n",
    "        nltk.Tree.fromstring(str(tree)).pretty_print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2cXz4qDqtaFD"
   },
   "source": [
    "Activity: Change the probabilities to assign the correct analysis for I washed the dishes in the sink"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lmj4_sJtvZrY"
   },
   "source": [
    "Getting the correct solution for both sentences at the same time requires an additional change to the form of the grammar. Any ideas what might work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1S4g-sorYNhc"
   },
   "source": [
    "## Treebanks and grammar induction\n",
    "\n",
    "Just writing these few small toy grammars has been quite involved. Writing full grammars that will have wide coverage is extremely difficult. We therefore learn them from corpora that have been annotated with syntax trees, known as treebanks.\n",
    "\n",
    "Some treebanks are build into NLTK and we can load an example as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0VA9K4C1USgc"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import treebank\n",
    "nltk.download('treebank')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H5-qKSNB33tr"
   },
   "source": [
    "We can inspect an example tree as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GWPcUrvfY8Jj"
   },
   "outputs": [],
   "source": [
    "t = treebank.parsed_sents('wsj_0001.mrg')[0]\n",
    "nltk.Tree.fromstring(str(t)).pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DTGYUaNJ4FbW"
   },
   "source": [
    "We can learn a grammar from treebank data as follows. \n",
    "\n",
    "First we have to make a slight change to the format of the trees:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ty4NfukUYfaM"
   },
   "outputs": [],
   "source": [
    "productions = []\n",
    "for item in treebank.fileids():\n",
    "  for tree in treebank.parsed_sents(item):\n",
    "    # perform optional tree transformations, e.g.:\n",
    "    tree.collapse_unary(collapsePOS = False)# Remove branches A-B-C into A-B+C\n",
    "    tree.chomsky_normal_form(horzMarkov = 2)# Remove A->(B,C,D) into A->B,C+D->D\n",
    "    productions += tree.productions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fsrPL2Xu4QJm"
   },
   "source": [
    "And then we can \"induce\" a probabilistic grammar as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rMGpNmBTVDxf"
   },
   "outputs": [],
   "source": [
    "from nltk import induce_pcfg, grammar \n",
    "S = grammar.Nonterminal('S')\n",
    "grammar_PCFG = induce_pcfg(S, productions)\n",
    "print(grammar_PCFG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qisoi2OwbcGK"
   },
   "outputs": [],
   "source": [
    "sentences = ['I drive in the city']\n",
    "parser = nltk.ViterbiParser(grammar_PCFG)\n",
    "import re\n",
    "for sent in sentences:\n",
    "    for tree in parser.parse_all(nltk.word_tokenize(sent)):\n",
    "        tree = re.sub(\"\\(p[^\\)]+\\)\",\"\",str(tree))\n",
    "        nltk.Tree.fromstring(str(tree)).pretty_print()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Week_11_Seminar.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
