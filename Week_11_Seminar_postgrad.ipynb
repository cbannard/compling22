{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "75f33f84",
      "metadata": {
        "id": "75f33f84"
      },
      "source": [
        "# LELA70331 Computational Linguistics Week 11\n",
        "\n",
        "This week we are going to take a look at Syntactic analysis, starting with part-of-speech tagging\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ceaa6b35-dc46-414f-9a24-d7063f3473f8",
      "metadata": {
        "id": "ceaa6b35-dc46-414f-9a24-d7063f3473f8"
      },
      "source": [
        "### Tagged corporaÂ¶\n",
        "In looking to understand part of speech tagging, it is useful to start by looking at some human (rather than machine) tagged data. NLTK contains a number of corpora. We can import one of these as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d43142a2-0a84-4010-a8f0-dab7153707f9",
      "metadata": {
        "id": "d43142a2-0a84-4010-a8f0-dab7153707f9"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('brown')\n",
        "from nltk.corpus import brown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08978ca3-00d8-401f-ac32-c12f5e5ea09a",
      "metadata": {
        "id": "08978ca3-00d8-401f-ac32-c12f5e5ea09a"
      },
      "outputs": [],
      "source": [
        "brown.tagged_words()[1:25]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba1c9e50-9f00-4351-980b-0d6e4b6d5b6d",
      "metadata": {
        "id": "ba1c9e50-9f00-4351-980b-0d6e4b6d5b6d"
      },
      "source": [
        "## Inspecting tagged corpora\n",
        "\n",
        "Inspecting human tagged corpora can be useful for both linguistic research and for building taggers. We can use the NLTK toolkit to do this. \n",
        "\n",
        "Most straightforwardly we can look at the frequency with which particular words are given a tag (we will return to this later when we come to build a tagger)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7207676f-5351-424e-acd3-a4912554739e",
      "metadata": {
        "id": "7207676f-5351-424e-acd3-a4912554739e"
      },
      "outputs": [],
      "source": [
        "sent = [(\"the\",\"DET\"),(\"man\",\"NOUN\"),(\"walked\",\"VERB\"),(\"the\",\"DET\"),(\"dog\",\"NOUN\")]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c4d4d85-b960-476d-9e01-cc85a7b43de6",
      "metadata": {
        "id": "3c4d4d85-b960-476d-9e01-cc85a7b43de6"
      },
      "outputs": [],
      "source": [
        "cfd1 = nltk.ConditionalFreqDist(sent)\n",
        "cfd1['the']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69ad31ac-0e1e-4ac5-a69c-bfd24489fe8f",
      "metadata": {
        "id": "69ad31ac-0e1e-4ac5-a69c-bfd24489fe8f"
      },
      "source": [
        "When we apply this to whole corpora, it becomes useful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca1ab27e-d4a2-4dba-84be-3efabcd67135",
      "metadata": {
        "id": "ca1ab27e-d4a2-4dba-84be-3efabcd67135"
      },
      "outputs": [],
      "source": [
        "brown_tagged = brown.tagged_words(tagset='universal')\n",
        "cfd1 = nltk.ConditionalFreqDist(brown_tagged)\n",
        "cfd1['the']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d3c3b51-8005-4c0d-a648-55409c3272ee",
      "metadata": {
        "id": "3d3c3b51-8005-4c0d-a648-55409c3272ee"
      },
      "source": [
        "We can extend this to look at the frequency with which particular word classes precede particular words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80495d08-c528-4e9e-b565-0d81b89dc694",
      "metadata": {
        "id": "80495d08-c528-4e9e-b565-0d81b89dc694"
      },
      "outputs": [],
      "source": [
        "brown_tagged = brown.tagged_words(tagset='universal')\n",
        "tags = [b[1] for (a, b) in nltk.bigrams(brown_tagged) if a[0] == 'the']\n",
        "fd = nltk.FreqDist(tags)\n",
        "fd.tabulate()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e4bf0ca-788e-4de3-9463-4a4115ca58cd",
      "metadata": {
        "id": "9e4bf0ca-788e-4de3-9463-4a4115ca58cd",
        "tags": []
      },
      "source": [
        "## Building an automatic tagger"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "581be5d1-91bf-481c-9717-e50945c87a57",
      "metadata": {
        "id": "581be5d1-91bf-481c-9717-e50945c87a57"
      },
      "source": [
        "A very simple approach to automated tagging that actually works quite well is to find the most common tag for each word in a training corpus (as we did above) and just tag all occurences of each word with its most common tag:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c97e3c4a-d291-466b-a639-2faf00961d3f",
      "metadata": {
        "id": "c97e3c4a-d291-466b-a639-2faf00961d3f"
      },
      "outputs": [],
      "source": [
        "brown_tagged_sents = brown.tagged_sents(tagset='universal')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b64f816-dbbf-40cb-992d-738ce553cc4f",
      "metadata": {
        "id": "4b64f816-dbbf-40cb-992d-738ce553cc4f"
      },
      "outputs": [],
      "source": [
        "unigram_tagger = nltk.UnigramTagger(brown_tagged_sents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b515475c-827d-4b1c-8520-341965a86616",
      "metadata": {
        "id": "b515475c-827d-4b1c-8520-341965a86616"
      },
      "outputs": [],
      "source": [
        "brown_tagged = brown.tagged_words(tagset='universal')\n",
        "tags = [b[1] for (a, b) in nltk.bigrams(brown_tagged) if a[0] == 'the']\n",
        "fd = nltk.FreqDist(tags)\n",
        "fd.tabulate()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0fa545a8-ee6e-43cb-b6fd-f66bf096cf8e",
      "metadata": {
        "id": "0fa545a8-ee6e-43cb-b6fd-f66bf096cf8e"
      },
      "source": [
        "We can formally evaluate this by splitting our data into a training set and a testing set. We obtain the by-word tag frequencies from the training set and evaluate by tagging the test set and comparing our predicted tags to the human tags."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1fc87145-debe-4060-88d5-c2d64b679d47",
      "metadata": {
        "id": "1fc87145-debe-4060-88d5-c2d64b679d47"
      },
      "outputs": [],
      "source": [
        "training_set_size = int(len(brown_tagged_sents) * 0.9)\n",
        "train_sents = brown_tagged_sents[:training_set_size]\n",
        "test_sents = brown_tagged_sents[training_set_size:]\n",
        "unigram_tagger = nltk.UnigramTagger(train_sents)\n",
        "unigram_tagger.evaluate(test_sents)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "628d08a2-47d0-4f8e-a06e-52a6f72073bf",
      "metadata": {
        "id": "628d08a2-47d0-4f8e-a06e-52a6f72073bf"
      },
      "source": [
        "### Regular expression based tagging\n",
        "\n",
        "As a next step we want to use a more intelligent way to deal with words we haven't seen before, but making use of their orthography and/or morphology. Write regular expressions to classify words in this way and see if you can improve performance. I've added one example rule to get you started."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1cc2d85b-6643-485a-8b3c-0590ecc5acd1",
      "metadata": {
        "id": "1cc2d85b-6643-485a-8b3c-0590ecc5acd1"
      },
      "outputs": [],
      "source": [
        "patterns = [\n",
        "    (r'.*ing$', 'VERB'),\n",
        "      ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6dcf0ac-735d-4b53-bf5d-b81e748a9356",
      "metadata": {
        "id": "e6dcf0ac-735d-4b53-bf5d-b81e748a9356"
      },
      "outputs": [],
      "source": [
        "t0 = nltk.DefaultTagger('NOUN')\n",
        "t1 = nltk.RegexpTagger(patterns, backoff=t0)\n",
        "t2 = nltk.UnigramTagger(train_sents, backoff=t1)\n",
        "t2.evaluate(test_sents)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1750cb6-b2c4-4d8d-90ef-adece63bd06b",
      "metadata": {
        "id": "c1750cb6-b2c4-4d8d-90ef-adece63bd06b"
      },
      "source": [
        "### Looking at the context"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8b60732-9e40-4112-b981-e6ce69e66c98",
      "metadata": {
        "id": "c8b60732-9e40-4112-b981-e6ce69e66c98"
      },
      "source": [
        "We want to improve this, and an obvious next step is to give the tag that is most frequent for this word when it follows the previous word. The problem is this doesn't do very well. Any idea why?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "717afde6-4433-4f64-95f9-18f0e6bd528d",
      "metadata": {
        "id": "717afde6-4433-4f64-95f9-18f0e6bd528d"
      },
      "outputs": [],
      "source": [
        "bigram_tagger = nltk.BigramTagger(train_sents)\n",
        "bigram_tagger.evaluate(test_sents)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27f88856-0d1e-45a6-8bfa-30af8d659fe4",
      "metadata": {
        "id": "27f88856-0d1e-45a6-8bfa-30af8d659fe4"
      },
      "source": [
        "We can still make use of the bigram information by combining it with the unigram tagger via a process known as backing off - for each word we check whether we have seen that word and preceding word in our training data. If we have then we tag it with the most frequent tag for that word in that context. If we haven't seen it then we tag the word with its most frequent tag regardless of context. And if we haven't seen the word before we tag it as a noun."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e46c806-7595-44d5-92b2-e101dd5bde37",
      "metadata": {
        "id": "3e46c806-7595-44d5-92b2-e101dd5bde37"
      },
      "outputs": [],
      "source": [
        "t0 = nltk.DefaultTagger('NOUN')\n",
        "t1 = nltk.UnigramTagger(train_sents, backoff=t0)\n",
        "t2 = nltk.BigramTagger(train_sents, backoff=t1)\n",
        "t2.evaluate(test_sents)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd3509bb-b468-41f7-9dfa-cc97fcd7bc60",
      "metadata": {
        "id": "bd3509bb-b468-41f7-9dfa-cc97fcd7bc60"
      },
      "source": [
        "### NLTK's Averaged Perceptron tagger\n",
        "\n",
        "NLTKs default prebuilt tagger uses a Perceptron just like that we have been using for other tasks on the module. For more information on this approach see here: https://explosion.ai/blog/part-of-speech-pos-tagger-in-python\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07ded4ac-9306-4e10-aefd-a4f0f08ebadd",
      "metadata": {
        "id": "07ded4ac-9306-4e10-aefd-a4f0f08ebadd"
      },
      "outputs": [],
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09637df3-1f57-4d85-afb7-84fca6a7d54f",
      "metadata": {
        "id": "09637df3-1f57-4d85-afb7-84fca6a7d54f"
      },
      "source": [
        "It can be run straightforwardly like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e276ad93-6cf6-4580-99a4-ed88cb859567",
      "metadata": {
        "id": "e276ad93-6cf6-4580-99a4-ed88cb859567"
      },
      "outputs": [],
      "source": [
        "text = nltk.word_tokenize(\"And now for something completely different\")\n",
        "nltk.pos_tag(text, tagset=\"universal\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa5747e5-289b-4464-b907-3447d93858e4",
      "metadata": {
        "id": "fa5747e5-289b-4464-b907-3447d93858e4"
      },
      "source": [
        "## Syntactic Parsing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a78b3548",
      "metadata": {
        "id": "a78b3548"
      },
      "source": [
        "We are going once again to use tools from NLTK, which we need to import as follows: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4f27ef1",
      "metadata": {
        "id": "a4f27ef1"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.parse.generate import generate\n",
        "from nltk import CFG, Tree\n",
        "nltk.download('punkt')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c889007c",
      "metadata": {
        "id": "c889007c"
      },
      "source": [
        "We can define phrase structure grammars using rewrite rules (see week 10 lecture for a definition) as follows: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ef28b3f",
      "metadata": {
        "id": "0ef28b3f"
      },
      "outputs": [],
      "source": [
        "grammar = nltk.CFG.fromstring(\"\"\"\n",
        "    S -> NP VP\n",
        "    NP -> Det N | Pronoun\n",
        "    VP -> V NP \n",
        "    Det -> 'the' \n",
        "    Pronoun -> 'I'\n",
        "    N -> 'dishes'  \n",
        "    V -> 'washed'\n",
        " \"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c72ef5c2",
      "metadata": {
        "id": "c72ef5c2"
      },
      "source": [
        "We can then \"parse\" tokenised input sentences as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8eef914d",
      "metadata": {
        "id": "8eef914d"
      },
      "outputs": [],
      "source": [
        "# define sentence and tokenize it\n",
        "sent = 'I washed the dishes'\n",
        "sent = nltk.word_tokenize(sent)\n",
        "# use a parser to generate all possible syntax trees for the input sentence given our grammar\n",
        "parser = nltk.ChartParser(grammar)\n",
        "# print out all analyses\n",
        "for tree in parser.parse(sent):\n",
        "    nltk.Tree.fromstring(str(tree)).pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05bb05d0",
      "metadata": {
        "id": "05bb05d0"
      },
      "source": [
        "And we can generate from the grammar as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "661e60b1",
      "metadata": {
        "id": "661e60b1"
      },
      "outputs": [],
      "source": [
        "for sentence in generate(grammar):\n",
        "     print(' '.join(sentence))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b5cf3a4",
      "metadata": {
        "id": "8b5cf3a4"
      },
      "source": [
        "Activity: Update the grammar so that it will parse \"They washed the car\". You can use the \"|\" symbol to allow multiple words or symbols on the right hand side of the rule, e.g. V -> 'washed' | 'threw'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a10ca861",
      "metadata": {
        "id": "a10ca861"
      },
      "source": [
        "Activity: Update the grammar so that it will parse \"The boy and his dog enter the park\". Note - it is permitted for the same terminal symbol to appear on the left and the right hand side of the same rule."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce3e492a",
      "metadata": {
        "id": "ce3e492a"
      },
      "source": [
        "Activity: Generate from the grammar again. Why does it crash?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9e4c92c",
      "metadata": {
        "id": "f9e4c92c"
      },
      "source": [
        "Activity: Update the grammar so that it will correctly parse the sentence \"I washed the dishes on the counter\". The intended interpretation is that the dishes were formerly on the counter and the washing took place in the sink. So the correct parse is as follows."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f80af1e",
      "metadata": {
        "id": "2f80af1e"
      },
      "source": [
        "![washed](https://drive.google.com/uc?id=15zyDad-tHMG3pevk9kxOv3upmyOmLGbk)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f8856bf",
      "metadata": {
        "id": "2f8856bf"
      },
      "outputs": [],
      "source": [
        "grammar = nltk.CFG.fromstring(\"\"\"\n",
        "    S -> NP VP\n",
        "    NP -> Det N | Pronoun\n",
        "    VP -> V NP \n",
        "    Det -> 'the' \n",
        "    Pronoun -> 'I'\n",
        "    N -> 'dishes' \n",
        "    V -> 'washed'\n",
        " \"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13147328",
      "metadata": {
        "id": "13147328"
      },
      "outputs": [],
      "source": [
        "sent = 'I washed the dishes on the counter'\n",
        "sent = nltk.word_tokenize(sent)\n",
        "parser = nltk.ChartParser(grammar)\n",
        "for tree in parser.parse(sent):\n",
        "    nltk.Tree.fromstring(str(tree)).pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40d3a2a7",
      "metadata": {
        "id": "40d3a2a7"
      },
      "source": [
        "Activity: now add rules to the same grammar to also give the correct analysis to the sentence \"I washed my hair in the shower\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa433fd4",
      "metadata": {
        "id": "fa433fd4"
      },
      "outputs": [],
      "source": [
        "sentences = ['I washed the dishes on the counter', 'I washed my hair in the shower']\n",
        "parser = nltk.ChartParser(grammar)\n",
        "for sent in sentences:\n",
        "    for tree in parser.parse(nltk.word_tokenize(sent)):\n",
        "        nltk.Tree.fromstring(str(tree)).pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01496ffb",
      "metadata": {
        "id": "01496ffb"
      },
      "source": [
        "# Probabilistic Grammar\n",
        "Because even very simple grammars can allow multiple, and sometimes a great many, analyses for simple sentences, particularly as the grammar gets big, it becomes necessary to find a way to prefer one parse over others. One way to accomplish this is with probabilistic grammars where a weight is given to each rule."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bbec1f91",
      "metadata": {
        "id": "bbec1f91"
      },
      "outputs": [],
      "source": [
        "grammar = nltk.PCFG.fromstring(\"\"\"\n",
        "    S -> NP VP [1.0]\n",
        "    NP -> Det N [0.25]\n",
        "    NP -> NP PP [0.25]\n",
        "    NP -> N PP [0.25]\n",
        "    NP -> Pronoun [0.25]\n",
        "    PP -> P NP [1.0]\n",
        "    VP -> V NP [0.5]\n",
        "    VP -> VP PP [0.5]\n",
        "    Det -> 'the' [0.5]\n",
        "    Det -> 'my' [0.5]\n",
        "    Pronoun -> 'I' [1.0]\n",
        "    N -> 'dishes'  [0.25]\n",
        "    N -> 'sink' [0.25]\n",
        "    N -> 'breakfast' [0.25]\n",
        "    N -> 'pyjamas'[0.25]\n",
        "    V -> 'washed' [0.5]\n",
        "    V ->  'ate' [0.5]\n",
        "    P -> 'in' [1.0]\n",
        " \"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb22ca80",
      "metadata": {
        "id": "bb22ca80"
      },
      "outputs": [],
      "source": [
        "sentences = ['I ate my breakfast in my pyjamas', 'I washed the dishes in the sink']\n",
        "parser = nltk.ViterbiParser(grammar)\n",
        "import re\n",
        "for sent in sentences:\n",
        "    for tree in parser.parse_all(nltk.word_tokenize(sent)):\n",
        "        tree = re.sub(\"\\(p[^\\)]+\\)\",\"\",str(tree))\n",
        "        nltk.Tree.fromstring(str(tree)).pretty_print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14c04449",
      "metadata": {
        "id": "14c04449"
      },
      "source": [
        "Activity: Change the probabilities to assign the correct analysis for I washed the dishes in the sink"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48f42e2a",
      "metadata": {
        "id": "48f42e2a"
      },
      "source": [
        "Getting the correct solution for both sentences at the same time requires an additional change to the form of the grammar. Any ideas what might work?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "832708ed",
      "metadata": {
        "id": "832708ed"
      },
      "source": [
        "## Treebanks and grammar induction\n",
        "\n",
        "Just writing these few small toy grammars has been quite involved. Writing full grammars that will have wide coverage is extremely difficult. We therefore learn them from corpora that have been annotated with syntax trees, known as treebanks.\n",
        "\n",
        "Some treebanks are build into NLTK and we can load an example as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3397618e",
      "metadata": {
        "id": "3397618e"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import treebank\n",
        "nltk.download('treebank')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90d43d0e",
      "metadata": {
        "id": "90d43d0e"
      },
      "source": [
        "We can inspect an example tree as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f695247",
      "metadata": {
        "id": "2f695247"
      },
      "outputs": [],
      "source": [
        "t = treebank.parsed_sents('wsj_0001.mrg')[0]\n",
        "nltk.Tree.fromstring(str(t)).pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c028094",
      "metadata": {
        "id": "2c028094"
      },
      "source": [
        "We can learn a grammar from treebank data as follows. \n",
        "\n",
        "First we have to make a slight change to the format of the trees:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74b456b5",
      "metadata": {
        "id": "74b456b5"
      },
      "outputs": [],
      "source": [
        "productions = []\n",
        "for item in treebank.fileids():\n",
        "  for tree in treebank.parsed_sents(item):\n",
        "    # perform optional tree transformations, e.g.:\n",
        "    tree.collapse_unary(collapsePOS = False)# Remove branches A-B-C into A-B+C\n",
        "    tree.chomsky_normal_form(horzMarkov = 2)# Remove A->(B,C,D) into A->B,C+D->D\n",
        "    productions += tree.productions()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6fc119f1",
      "metadata": {
        "id": "6fc119f1"
      },
      "source": [
        "And then we can \"induce\" a probabilistic grammar as follows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5df97230",
      "metadata": {
        "id": "5df97230"
      },
      "outputs": [],
      "source": [
        "from nltk import induce_pcfg, grammar \n",
        "S = grammar.Nonterminal('S')\n",
        "grammar_PCFG = induce_pcfg(S, productions)\n",
        "print(grammar_PCFG)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f569131f",
      "metadata": {
        "id": "f569131f"
      },
      "outputs": [],
      "source": [
        "sentences = ['I drive in the city']\n",
        "parser = nltk.ViterbiParser(grammar_PCFG)\n",
        "import re\n",
        "for sent in sentences:\n",
        "    for tree in parser.parse_all(nltk.word_tokenize(sent)):\n",
        "        tree = re.sub(\"\\(p[^\\)]+\\)\",\"\",str(tree))\n",
        "        nltk.Tree.fromstring(str(tree)).pretty_print()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Week_11_Seminar.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}