{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ag1DD6P8qBVx"
      },
      "source": [
        "# LELA70331 Computational Linguistics Week 7\n",
        "\n",
        "This week we are going to take a look at dialogue systems - first chatbots and then task-based dialogue systems"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59GicpRsJft0"
      },
      "source": [
        "## Rule-based chatbot: Eliza"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xr7KjD8uJft1"
      },
      "source": [
        "As described in this week's lecture, Eliza is a chatbot that simulates a Rogerian therapist, making use of a set of rules in the form of regular expressions. At the heart of Eliza are the substitution function (re.sub in Python) and grouping. We'll start with a quick recap as to what these are.\n",
        "\n",
        "We need to import the Regular Expressions module in Python."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5NFz8-e2Jft1"
      },
      "outputs": [],
      "source": [
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cm72Ki4SJft2"
      },
      "source": [
        "This gives as access to the very useful function re.sub."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "IBpQbrgzJft2"
      },
      "source": [
        "### re.sub()\n",
        "\n",
        "This finds all occurences of a given sequence and replaces it with a sequence provided:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GcUXWQqZJft3"
      },
      "outputs": [],
      "source": [
        "utt = 'walked'\n",
        "re.sub('ed','ing',utt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0SqH7z1JJft3"
      },
      "source": [
        "### Groups\n",
        "\n",
        "Grouping is a very powerful technique for picking out substrings from a string that matches a specified pattern. Parentheses are used to indicate the start and end of the substring. It is very powerful when combined with substitution.\n",
        "\n",
        "You can use parentheses to capture a particular substring within a pattern and then use it in your replacement string within sub. For example:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "T3B8ueo2Jft4"
      },
      "outputs": [],
      "source": [
        "utt = \"procrastinating\"\n",
        "re.sub('([a-z]+)ing','\\\\1ed',utt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5t0D4AbcJft4"
      },
      "source": [
        "## A very simple Elizabot\n",
        "\n",
        "The code below implements a very simple Eliza. The function respond takes an utterance as input and using re.sub to generate responses. The loop below the function creates a simple interface that takes user input and prints the response.\n",
        "\n",
        "We can extend Eliza's ability by adding additional rules."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bFv8yrpylNuU"
      },
      "outputs": [],
      "source": [
        "def respond(utt):\n",
        "  utt = re.sub('hello my name is (.+)','hello \\\\1 how are you feeling today', utt)\n",
        "  return utt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dcIjRBRKlKfi"
      },
      "outputs": [],
      "source": [
        "utt = \"\"\n",
        "while utt != 'goodbye':\n",
        "    utt = input('> ')\n",
        "    reply = respond(utt)\n",
        "    if reply != utt:\n",
        "        print(reply)\n",
        "    else:\n",
        "        if utt != \"goodbye\":\n",
        "            print(\"Can you rephrase that?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUEniCI2qK-X"
      },
      "source": [
        "### Activity\n",
        "\n",
        "Add patterns (using substitutions and grouping) to the respond function that will allow Eliza to conduct both of these conversations. Test your system by conducting the conversation with Eliza.\n",
        "\n",
        "User: hello my name is emma <br>\n",
        "Eliza: Hello emma my name is Eliza. How are you feeling today? <br>\n",
        "User: i am feeling very happy <br>\n",
        "Eliza: Do you often feel happy? <br>\n",
        "User: yes since I started my new job <br>\n",
        "Eliza: Can you tell me about starting your new job? <br>\n",
        "\n",
        "User: hello my name is john <br>\n",
        "Eliza: Hello john, my name is Eliza. How are you feeling today? <br>\n",
        "User: i am feeling pretty happy <br>\n",
        "Eliza: Do you often feel happy? <br>\n",
        "User: yes since I moved house <br>\n",
        "Eliza: Can you tell me about moving house? <br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4M2vBC1Jft5"
      },
      "source": [
        "### Reverse engineering the NLTK Eliza\n",
        "\n",
        "There have been many implementations of Eliza over the years. One version is built into the NLTK toolkit. This can be run as follows:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o1fqK44ethJ3"
      },
      "outputs": [],
      "source": [
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4yOeuTVotfQJ",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "nltk.chat.eliza.demo()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGBMUgbLJft6"
      },
      "source": [
        "Activity to try in your own time: Conduct a four-turn-each conversation of your own with the NLTK Eliza. Adds the substitution that you think Eliza is using to generate the responses to your own chatbot using the code below. Where you find Eliza's response to be lacking, update the substitution to give a better response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "cCEuXt6lJft6"
      },
      "outputs": [],
      "source": [
        "def respond(utt):\n",
        "  utt = re.sub('hello my name is (.+)','hello \\\\1 how are you feeling today', utt)\n",
        "  return utt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VvRImiJgJft6"
      },
      "outputs": [],
      "source": [
        "utt = \"\"\n",
        "while utt != 'goodbye':\n",
        "    utt = input('> ')\n",
        "    reply = respond(utt)\n",
        "    if reply != utt:\n",
        "        print(reply)\n",
        "    else:\n",
        "        if utt != \"goodbye\":\n",
        "            print(\"Can you rephrase that?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Y_MWBVfJft7"
      },
      "source": [
        "### Text Generation\n",
        "\n",
        "We are now going to look at methods for creative generation of text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqfV1aIxJft7"
      },
      "source": [
        "### Ngram-based generation\n",
        "The simplest way to perform generation is to learn and apply n-gram probabilities. The way this works is that we first calculate the probability of each word in a corpus following each other word. We then pick a random word to start the sentence and the select a next word that is probable given that word. We then output that next word and then select another word that is probable given THAT word. We then repeat for N words.\n",
        "\n",
        "This allows us to generate sentences that look a little like natural language, and to imitate particular genres and styles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hbHRbA98Jft7"
      },
      "outputs": [],
      "source": [
        "import nltk\t\n",
        "nltk.download('gutenberg')\n",
        "nltk.corpus.gutenberg.fileids()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8rXMFYOWJft7"
      },
      "outputs": [],
      "source": [
        "kjv = nltk.corpus.gutenberg.words('bible-kjv.txt')\n",
        "macbeth = nltk.corpus.gutenberg.words('shakespeare-macbeth.txt')\n",
        "bigrams = nltk.bigrams(kjv)\n",
        "cfd = nltk.ConditionalFreqDist(bigrams)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rLTMehq3Jft7"
      },
      "outputs": [],
      "source": [
        "cfd['the']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KLsBT2AJJft8"
      },
      "outputs": [],
      "source": [
        "def generate_argmax(cfdist, word, num=15):\n",
        "    for i in range(num):\n",
        "        print(word, end=' ')\n",
        "        word = cfdist[word].max()\n",
        "import random\n",
        "def generate_random(cfdist, word, num=15):\n",
        "    for i in range(num):\n",
        "        print(word, end=' ')\n",
        "        word = random.choices(list(cfd[word].keys()), weights=cfd[word].values(), k=1)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F5EU3KPIJft8"
      },
      "outputs": [],
      "source": [
        "generate_argmax(cfd,\"the\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8nnb8nBaJft8"
      },
      "source": [
        "### Text generation with neural language models\n",
        "The example above uses a very simple language model. However much better performance can be achieved using the same basic principle but employing neural network-based language models and huge amounts of data. We are going to use a pretrained model called GPT-2 (https://openai.com/blog/better-language-models/) which has been made publically available and is very widely used. It is a neural network model using an architecture known as a transformer. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PZ7p7xYWJft8"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-aHpTMqfJft8"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2TokenizerFast, GPT2LMHeadModel, set_seed\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eB7ECbEiJft8"
      },
      "outputs": [],
      "source": [
        "model_name = 'gpt2-medium'\n",
        "num_samples = 1\n",
        "max_length = 100\n",
        "top_k = 10 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bX80KiBIJft8"
      },
      "outputs": [],
      "source": [
        "set_seed(42)\n",
        "tokenizer = GPT2TokenizerFast.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name, pad_token_id=tokenizer.eos_token_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s0H3BmPHJft8"
      },
      "outputs": [],
      "source": [
        "prompts = []\n",
        "prompt = input()\n",
        "prompts.append(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R75LNb3FJft8"
      },
      "outputs": [],
      "source": [
        "output = {}\n",
        "for text in prompts:\n",
        "\tinput_ids = tokenizer.encode(text, return_tensors='pt')\n",
        "\tmax_length = len(text.split()) + max_length\n",
        "\tresponses = []\n",
        "\tresponses = model.generate(input_ids, max_length = max_length, do_sample=True, top_k=top_k, num_return_sequences=num_samples)\n",
        "\n",
        "\tresponses = responses[:, input_ids.shape[-1]:]\n",
        "\toutput[text] = []\n",
        "\tfor i, r in enumerate(responses):\n",
        "\t\tresponse = tokenizer.decode(r, skip_special_tokens=True)\n",
        "\t\toutput[text].append(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-n0cMrobJft9"
      },
      "outputs": [],
      "source": [
        "for k in output:\n",
        "    print('prompt: ' + k + '\\n')\n",
        "    for i, r in enumerate(output[k]):\n",
        "        print('================================== Output ==================================')\n",
        "        print(k + \" \" + r.strip() + '\\n')\n",
        "    print('********************************************************************************************************************')\n",
        "    print('')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIDS4M25uKXP"
      },
      "source": [
        "## Corpus-based chatbots"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIUnZjlxsUj1"
      },
      "source": [
        "Training and running a corpus-based chatbot takes more steps than we have time for today. If you want to have a go in your own time, then you will find a tutorial for doing so in Pytorch here:\n",
        "\n",
        "https://pytorch.org/tutorials/beginner/chatbot_tutorial.html\n",
        "\n",
        "There is a link at the top of the page to open the notebook in Colab.\n",
        "\n",
        "To run this in colab easily just put the following code block at the top and run it before working through the rest of the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eYlQ27c6Jft9"
      },
      "outputs": [],
      "source": [
        "! wget https://zissou.infosci.cornell.edu/convokit/datasets/movie-corpus/movie-corpus.zip\n",
        "! unzip movie-corpus.zip\n",
        "! mkdir data\n",
        "! mv movie-corpus data/"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can try the Facebook Corpus-based chatbot (Blenderbot) here: https://link.ainize.ai/3z6sgq2"
      ],
      "metadata": {
        "id": "NTtgJqyxJ7uc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Intent classification\n",
        "\n",
        "We are now going to at task-based dialogue systems and specifically the sub-task of intent classification (the focus of your coursework). You should write rules to uniquely and correctly identify each of the following utterances:\n",
        "\n",
        "PlayMusic: play the weather girls\n",
        "\n",
        "AddToPlaylist: add this to my italian film soundtrack playlist\n",
        "\n",
        "RateBook: give the restaurant guidebook 5 stars\n",
        "\n",
        "SearchScreeningEvent: find screenings of the book thief at around 7\n",
        "\n",
        "BookRestaurant: book me a table outside for 2 for dinner at the national theatre restaurant\n",
        "\n",
        "GetWeather: will it be warm enough to eat dinner outside at around 7 tonight\n",
        "\n",
        "SearchCreativeWork: find me songs films or books about restaurants\n",
        "\n",
        "Here is the function from your coursework notebook:"
      ],
      "metadata": {
        "id": "ogkfOFbaJ8q7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def assign_intent(utt, verbose=False):\n",
        "  PlayMusic_Pattern = re.compile(\"play|music\")\n",
        "  AddToPlaylist_Pattern = re.compile(\"add|playlist\")\n",
        "  RateBook_Pattern = re.compile(\"rate|book\")\n",
        "  SearchScreeningEvent_Pattern = re.compile(\"screening\")\n",
        "  BookRestaurant_Pattern = re.compile(\"book|restaurant|food\")\n",
        "  GetWeather_Pattern = re.compile(\"get|weather\")\n",
        "  SearchCreativeWork_Pattern = re.compile(\"creative\")\n",
        " \n",
        "  weights = {}\n",
        "  weights['PlayMusic'] = len(re.findall(PlayMusic_Pattern,  utt))\n",
        "  weights['AddToPlaylist'] = len(re.findall(AddToPlaylist_Pattern,  utt))\n",
        "  weights['RateBook'] = len(re.findall(RateBook_Pattern,  utt))\n",
        "  weights['SearchScreeningEvent'] = len(re.findall(SearchScreeningEvent_Pattern,  utt))\n",
        "  weights['BookRestaurant'] = len(re.findall(BookRestaurant_Pattern,  utt))\n",
        "  weights['GetWeather'] = len(re.findall(GetWeather_Pattern,  utt))\n",
        "  weights['SearchCreativeWork'] = len(re.findall(SearchCreativeWork_Pattern,  utt))\n",
        "  if verbose:\n",
        "      print(weights)\n",
        "  if max(weights.values()) == 0:\n",
        "      return random.choice(list(weights.keys()))\n",
        "  else:\n",
        "      weights_as_list = list(weights.items())\n",
        "      random.shuffle(weights_as_list)\n",
        "      weights=dict(weights_as_list)\n",
        "      return max(weights, key=lambda key: weights[key])"
      ],
      "metadata": {
        "id": "N9RtO7-2Ht-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_inputs = ['play the weather girls','add this to my italian film soundtrack playlist','give the restaurant guidebook 5 stars','find screenings of the book thief at around 7','book me a table outside for 2 for dinner at the national theatre restaurant','will it be warm enough to eat dinner outside at around 7 tonight','find me songs films or books about restaurants']\n",
        "[print(str(assign_intent(utt)) + \" : \" + utt) for utt in example_inputs]"
      ],
      "metadata": {
        "id": "QwEtGrEWHs10"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Week_7_Seminar.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}