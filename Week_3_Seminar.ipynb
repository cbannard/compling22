{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ada7675f",
   "metadata": {
    "id": "ada7675f"
   },
   "source": [
    "# LELA32051 Computational Linguistics Week 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89b8c5b",
   "metadata": {},
   "source": [
    "We will start by installing and importing some functions and tools that we need for today's session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede3dfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install annoy\n",
    "!pip install torch torchvision \n",
    "!wget https://www.dropbox.com/s/0kuv1219ith5a9e/week3tools.py\n",
    "import week3tools\n",
    "from week3tools import EmbeddingUtil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from google.colab import output\n",
    "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "!unzip -q glove.6B.zip\n",
    "embeddings = EmbeddingUtil.from_embeddings_file('glove.6B.100d.txt')\n",
    "output.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243df473",
   "metadata": {},
   "source": [
    "## Some more on regular expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6057638d",
   "metadata": {},
   "source": [
    "### Combining sub with groups\n",
    "The re.sub function and grouping become particularly powerful when they are combined. You can use parentheses to capture a particular substring within a pattern and then use it in your replacement string within sub. For example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91963cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "opening_sentence = \"a young man came out of the garret in which he lodged in S. Place and walked slowly towards K. bridge.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33d2ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "re.sub('([a-z]+)ed','is \\\\1ing',opening_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef174bd",
   "metadata": {},
   "source": [
    "Activity: Use sub combined with groups to convert the sentence \"man bites dog\" into \"dog bites man\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2a83d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"man bites dog\"\n",
    "print(re.sub('','',sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8070f9e1",
   "metadata": {},
   "source": [
    "Activity: Use sub combined with groups to convert the sentence \"man strokes dog\" into \"dog is stroked by man\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1588f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"man strokes dog\"\n",
    "print(re.sub('','',sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53a7a48",
   "metadata": {},
   "source": [
    "### re.split()\n",
    "In week 1 we learned to tokenise a string using the string function split. re also has a split function. re.split() takes a regular expression as a first argument (unless you have a precompiled pattern) and a string as second argument, and split the string into tokens divided by all substrings matched by the regular expression. \n",
    "Can you improve on the following tokeniser? (clue: you might need to add a re.sub statement)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc85b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5543015b",
   "metadata": {},
   "outputs": [],
   "source": [
    "opening_sentence = \"On an exceptionally hot evening early in July a young man came out of the garret in which he lodged in S. Place and walked slowly, as though in hesitation, towards K. bridge.\"\n",
    "to_split_on = re.compile(\" \")\n",
    "opening_sentence_new = to_split_on.split(opening_sentence)\n",
    "print(opening_sentence_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f508cffb",
   "metadata": {},
   "source": [
    "### Escaping special characters\n",
    "We have learned about a number of character that have a special meaning in regular expressions (periods, dollar signs etc). We might sometimes want to search for these characters in strings. To do this we can \"escape\" the character using a backslash() as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02871a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall(\"\\.\",opening_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91482388",
   "metadata": {},
   "source": [
    "# Iterating/for loops\n",
    "\n",
    "Humans reading texts do so one word at a time. The same is often true for computers. This is most commonly performed using a \"for loop\". This can be straightforwardly implemented for lists. In the following code we iterate through the list printing each entry as we go. Note that the end=\"\" in the print statement tells it to end each printed token with a space rather than a new line which is the default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d2778d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in opening_sentence_new:\n",
    "    print(word, end=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89df10d5",
   "metadata": {},
   "source": [
    "You will notice that in the loop above the print statement is indented. We say that a statement that occurs within a loop is nested within that loop. Any statement that is nested inside another has to be indented in Python. The standard way to indent is to use 4 spaces, although you can also use a tab.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb5bc5b",
   "metadata": {},
   "source": [
    "# Sentence Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48df58a8",
   "metadata": {},
   "source": [
    "Above we split a sentence into words. However most texts that we want to process have more than one sentence, so we also need to segment text into sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1dd2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://www.gutenberg.org/files/2554/2554-0.txt \n",
    "f = open('2554-0.txt')\n",
    "raw = f.read()\n",
    "chapter_one = raw[5464:23725]\n",
    "chapter_one = re.sub('\\n',' ',chapter_one)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f08861",
   "metadata": {},
   "source": [
    "In the following code, REPLACE the patterns for splitting into sentences and into words, in order to produce a well segmented and tokenised text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee10b785",
   "metadata": {},
   "outputs": [],
   "source": [
    "C_and_P=[]\n",
    "to_split_on_sent = re.compile(\"b\")\n",
    "to_split_on_word = re.compile(\"a\")\n",
    "C_and_P_sentences = to_split_on_sent.split(chapter_one)\n",
    "\n",
    "for sent in C_and_P_sentences:\n",
    "    C_and_P.append(to_split_on_word.split(sent.lstrip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619c29c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(C_and_P)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904101cb-91ef-47ab-b918-a7ba7e743bf5",
   "metadata": {
    "id": "904101cb-91ef-47ab-b918-a7ba7e743bf5"
   },
   "source": [
    "# Vector semantics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8Zj1DoDsYQQV",
   "metadata": {
    "id": "8Zj1DoDsYQQV"
   },
   "source": [
    "In this week's lecture you heard about Vector-based semantics. Today we will take a look at these models in Python. First we will build a co-occurence model from our segmented and tokenized chapter of Crime and Punishment using an imported function. This function allows us to specify the window that we use as context. We will use a window size of 5 words either side of each word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803b1b51-5c1c-4090-ae2e-083011a44ef0",
   "metadata": {
    "id": "803b1b51-5c1c-4090-ae2e-083011a44ef0"
   },
   "outputs": [],
   "source": [
    "M_co_occurrence, word2Ind_co_occurrence = week3tools.compute_co_occurrence_matrix(C_and_P, window_size=5)\n",
    "\n",
    "semantic_space = pd.DataFrame(M_co_occurrence, index=word2Ind_co_occurrence.keys(), columns=word2Ind_co_occurrence.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9475be-3e0e-4705-b814-75e59455b11b",
   "metadata": {
    "id": "2c9475be-3e0e-4705-b814-75e59455b11b"
   },
   "source": [
    "We can look at the size of the matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8e37e6-f1af-4606-b386-4a15bfffb333",
   "metadata": {
    "id": "4a8e37e6-f1af-4606-b386-4a15bfffb333"
   },
   "outputs": [],
   "source": [
    "semantic_space.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d389f36b-2292-40f0-9edd-961e215d8f7d",
   "metadata": {
    "id": "d389f36b-2292-40f0-9edd-961e215d8f7d"
   },
   "source": [
    "We can look at a part of the semantic space like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438e23db-cb06-4493-831f-069d552b9f9a",
   "metadata": {
    "id": "438e23db-cb06-4493-831f-069d552b9f9a"
   },
   "outputs": [],
   "source": [
    "semantic_space.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555e8642-293f-4b37-a3d4-20b3e0ad89bf",
   "metadata": {
    "id": "555e8642-293f-4b37-a3d4-20b3e0ad89bf"
   },
   "source": [
    "And another example part like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480e5de6-2a00-4596-b8d9-970322c8d33b",
   "metadata": {
    "id": "480e5de6-2a00-4596-b8d9-970322c8d33b"
   },
   "outputs": [],
   "source": [
    "semantic_space.iloc[200:220,200:220]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ca3055-2046-40fe-a198-6dbf342d6bd8",
   "metadata": {
    "id": "83ca3055-2046-40fe-a198-6dbf342d6bd8"
   },
   "source": [
    "## Using our vectors (and pretrained embeddings)\n",
    "\n",
    "Vectors are best when learned from very large text collections. However learning such vectors, particular using neural network methods, is very computationally intensive. As a result most people make use of pretrained embeddings such as those found at\n",
    "\n",
    "https://code.google.com/archive/p/word2vec/\n",
    "\n",
    "or\n",
    "\n",
    "https://nlp.stanford.edu/projects/glove/\n",
    "\n",
    "At the top of the notebook we imported the latter of these and can now use them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf9a9d2-eff9-47aa-9e85-aad713a6c19c",
   "metadata": {
    "id": "baf9a9d2-eff9-47aa-9e85-aad713a6c19c"
   },
   "outputs": [],
   "source": [
    "vec=embeddings.get_embedding(\"child\")\n",
    "print(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382d5835-77ed-4fb4-9452-cae913ebbc61",
   "metadata": {
    "id": "382d5835-77ed-4fb4-9452-cae913ebbc61"
   },
   "outputs": [],
   "source": [
    "embeddings.get_closest_to_vector(vec, n=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e0b00f-2595-4402-8492-54e0ee364824",
   "metadata": {
    "id": "47e0b00f-2595-4402-8492-54e0ee364824"
   },
   "source": [
    "Another semantic property of embeddings is their ability to capture relational meanings. In an important early vector space model of cognition, Rumelhart and Abrahamson (1973) proposed the parallelogram model for solving simple analogy problems of the form a is to b as a* is to what?. In such problems, a system given a problem like apple:tree::grape:?, i.e., apple is to tree as  grape is to , and must fill in the word vine.\n",
    "\n",
    "In the parallelogram model, the vector from the word apple to the word tree (= apple − tree) is added to the vector for grape (grape); the nearest word to that point is returned. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065739ee-7ef2-4471-b33e-6d61c41cf7f2",
   "metadata": {
    "id": "065739ee-7ef2-4471-b33e-6d61c41cf7f2"
   },
   "outputs": [],
   "source": [
    "embeddings.compute_and_print_analogy('fly', 'plane', 'sail')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Week_3_Seminar.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
